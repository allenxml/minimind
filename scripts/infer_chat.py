"""
MiniMind äº¤äº’å¼å¯¹è¯æ¨ç†è„šæœ¬

æœ¬è„šæœ¬æä¾›äº†ä¸€ä¸ªå‘½ä»¤è¡Œäº¤äº’ç•Œé¢ï¼Œç”¨äºä¸ MiniMind æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚

ä¸»è¦åŠŸèƒ½:
1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
2. æä¾›äº¤äº’å¼å¯¹è¯ç•Œé¢
3. æ”¯æŒæµå¼è¾“å‡ºï¼ˆé€å­—æ˜¾ç¤ºï¼‰
4. æ”¯æŒå¤šè½®å¯¹è¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰
5. æ”¯æŒ LoRA æƒé‡åŠ è½½

ä½¿ç”¨æ–¹æ³•:
    # åŸºç¡€å¯¹è¯
    python infer_chat.py
    
    # ä½¿ç”¨ç‰¹å®šæ¨¡å‹
    python infer_chat.py --model_path ../out/full_sft_512.pth
    
    # ä½¿ç”¨ LoRA
    python infer_chat.py --lora_path ../out/lora/lora_identity_512.pth
    
    # æ¨ç†æ¨¡å¼ï¼ˆå¸¦æ€è€ƒè¿‡ç¨‹ï¼‰
    python infer_chat.py --reasoning 1

äº¤äº’å‘½ä»¤:
- è¾“å…¥é—®é¢˜åæŒ‰å›è½¦å‘é€
- è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²
- è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºç¨‹åº
- Ctrl+C å¼ºåˆ¶é€€å‡º
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import torch
from transformers import AutoTokenizer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import apply_lora, load_lora


def load_model(args):
    """
    åŠ è½½æ¨¡å‹å’Œ tokenizer
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        Tuple[model, tokenizer]: åŠ è½½å¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨
    """
    # åˆ›å»ºæ¨¡å‹é…ç½®
    lm_config = MiniMindConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        use_moe=bool(args.use_moe),
        flash_attn=args.flash_attn
    )
    
    # åŠ è½½ tokenizer
    tokenizer_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'model')
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    
    # åˆ›å»ºæ¨¡å‹
    model = MiniMindForCausalLM(lm_config)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    if os.path.exists(args.model_path):
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹æƒé‡: {args.model_path}")
        weights = torch.load(args.model_path, map_location=args.device)
        model.load_state_dict(weights, strict=False)
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æƒé‡: {args.model_path}")
        print("   ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    # åŠ è½½ LoRA æƒé‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.lora_path and os.path.exists(args.lora_path):
        print(f"ğŸ“‚ åŠ è½½ LoRA æƒé‡: {args.lora_path}")
        apply_lora(model)
        load_lora(model, args.lora_path)
    
    # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = model.to(args.device)
    model.eval()
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params / 1e6:.2f} M")
    
    return model, tokenizer


def generate_response(model, tokenizer, messages, args):
    """
    ç”Ÿæˆæ¨¡å‹å›å¤
    
    Args:
        model: è¯­è¨€æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        messages: å¯¹è¯å†å²
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        str: æ¨¡å‹ç”Ÿæˆçš„å›å¤
    """
    # ä½¿ç”¨ chat_template æ ¼å¼åŒ–å¯¹è¯
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        add_special_tokens=False
    ).to(args.device)
    
    # ç”Ÿæˆå›å¤
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=args.max_new_tokens,
            do_sample=args.do_sample,
            temperature=args.temperature,
            top_p=args.top_p,
            top_k=args.top_k,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç ç”Ÿæˆçš„ token
    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return response


def stream_generate(model, tokenizer, messages, args):
    """
    æµå¼ç”Ÿæˆæ¨¡å‹å›å¤ï¼ˆé€å­—è¾“å‡ºï¼‰
    
    Args:
        model: è¯­è¨€æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        messages: å¯¹è¯å†å²
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Yields:
        str: æ¯æ¬¡ç”Ÿæˆçš„æ–° token
    """
    # ä½¿ç”¨ chat_template æ ¼å¼åŒ–å¯¹è¯
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        add_special_tokens=False
    ).to(args.device)
    
    input_ids = inputs['input_ids']
    past_key_values = None
    generated_tokens = []
    
    # é€ token ç”Ÿæˆ
    for _ in range(args.max_new_tokens):
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                past_key_values=past_key_values,
                use_cache=True
            )
        
        # è·å–ä¸‹ä¸€ä¸ª token çš„ logits
        next_token_logits = outputs.logits[:, -1, :]
        
        # é‡‡æ ·æˆ–è´ªå©ªè§£ç 
        if args.do_sample:
            # åº”ç”¨æ¸©åº¦
            next_token_logits = next_token_logits / args.temperature
            
            # Top-k é‡‡æ ·
            if args.top_k > 0:
                indices_to_remove = next_token_logits < torch.topk(next_token_logits, args.top_k)[0][..., -1, None]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Top-p é‡‡æ ·
            if args.top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > args.top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                next_token_logits[indices_to_remove] = float('-inf')
            
            # é‡‡æ ·
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
        else:
            # è´ªå©ªè§£ç 
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº† EOS
        if next_token.item() == tokenizer.eos_token_id:
            break
        
        # è§£ç å¹¶è¾“å‡º
        token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)
        generated_tokens.append(next_token.item())
        yield token_text
        
        # æ›´æ–°è¾“å…¥
        input_ids = next_token
        past_key_values = outputs.past_key_values
    
    return ''.join(tokenizer.decode(generated_tokens, skip_special_tokens=True))


def chat_loop(model, tokenizer, args):
    """
    äº¤äº’å¼å¯¹è¯å¾ªç¯
    
    Args:
        model: è¯­è¨€æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print("\n" + "="*60)
    print("ğŸ¤– MiniMind å¯¹è¯ç³»ç»Ÿ")
    print("="*60)
    print("ğŸ’¡ æç¤º:")
    print("   - è¾“å…¥é—®é¢˜åæŒ‰å›è½¦å‘é€")
    print("   - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("   - è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("="*60 + "\n")
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    messages = []
    
    # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
    if args.system_prompt:
        messages.append({"role": "system", "content": args.system_prompt})
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()
            
            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            if not user_input:
                continue
            elif user_input.lower() in ['exit', 'quit']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif user_input.lower() == 'clear':
                messages = []
                if args.system_prompt:
                    messages.append({"role": "system", "content": args.system_prompt})
                print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º\n")
                continue
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": user_input})
            
            # ç”Ÿæˆå›å¤
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            
            if args.stream:
                # æµå¼è¾“å‡º
                response_parts = []
                for token in stream_generate(model, tokenizer, messages, args):
                    print(token, end="", flush=True)
                    response_parts.append(token)
                response = ''.join(response_parts)
            else:
                # ä¸€æ¬¡æ€§è¾“å‡º
                response = generate_response(model, tokenizer, messages, args)
                print(response)
            
            print("\n")
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            messages.append({"role": "assistant", "content": response})
            
            # é™åˆ¶å†å²é•¿åº¦ï¼ˆé˜²æ­¢ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
            if len(messages) > args.max_history * 2 + 1:  # +1 for system prompt
                # ä¿ç•™ç³»ç»Ÿæç¤ºå’Œæœ€è¿‘çš„å¯¹è¯
                if args.system_prompt:
                    messages = messages[:1] + messages[-(args.max_history * 2):]
                else:
                    messages = messages[-(args.max_history * 2):]
                    
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            continue


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MiniMind äº¤äº’å¼å¯¹è¯")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_path", type=str, default="../out/full_sft_512.pth",
                        help="æ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument("--lora_path", type=str, default=None,
                        help="LoRA æƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--hidden_size", type=int, default=512,
                        help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_hidden_layers", type=int, default=8,
                        help="éšè—å±‚æ•°é‡")
    parser.add_argument("--use_moe", type=int, default=0, choices=[0, 1],
                        help="æ˜¯å¦ä½¿ç”¨ MoE æ¶æ„")
    parser.add_argument("--flash_attn", type=int, default=1, choices=[0, 1],
                        help="æ˜¯å¦ä½¿ç”¨ Flash Attention")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_new_tokens", type=int, default=512,
                        help="æœ€å¤§ç”Ÿæˆ token æ•°")
    parser.add_argument("--do_sample", type=int, default=1, choices=[0, 1],
                        help="æ˜¯å¦ä½¿ç”¨é‡‡æ ·ï¼ˆ0=è´ªå©ªè§£ç ï¼‰")
    parser.add_argument("--temperature", type=float, default=0.7,
                        help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top_p", type=float, default=0.9,
                        help="Top-p é‡‡æ ·å‚æ•°")
    parser.add_argument("--top_k", type=int, default=50,
                        help="Top-k é‡‡æ ·å‚æ•°")
    
    # å¯¹è¯å‚æ•°
    parser.add_argument("--system_prompt", type=str, default="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚",
                        help="ç³»ç»Ÿæç¤ºè¯")
    parser.add_argument("--max_history", type=int, default=10,
                        help="ä¿ç•™çš„æœ€å¤§å¯¹è¯è½®æ•°")
    parser.add_argument("--stream", type=int, default=1, choices=[0, 1],
                        help="æ˜¯å¦æµå¼è¾“å‡º")
    parser.add_argument("--reasoning", type=int, default=0, choices=[0, 1],
                        help="æ˜¯å¦ä½¿ç”¨æ¨ç†æ¨¡å¼")
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument("--device", type=str, 
                        default="cuda:0" if torch.cuda.is_available() else "cpu",
                        help="è¿è¡Œè®¾å¤‡")
    
    args = parser.parse_args()
    
    # æ¨ç†æ¨¡å¼çš„ç³»ç»Ÿæç¤º
    if args.reasoning:
        args.system_prompt = "ä½ æ˜¯ä¸€ä¸ªå–„äºæ€è€ƒçš„AIåŠ©æ‰‹ã€‚åœ¨å›ç­”é—®é¢˜æ—¶ï¼Œè¯·å…ˆåœ¨<think>æ ‡ç­¾ä¸­è¿›è¡Œæ€è€ƒï¼Œç„¶ååœ¨<answer>æ ‡ç­¾ä¸­ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args)
    
    # å¼€å§‹å¯¹è¯
    chat_loop(model, tokenizer, args)