"""
MiniMind æ¨¡å‹è¯„ä¼°ä¸æ¨ç†è„šæœ¬

æœ¬è„šæœ¬æä¾›äº† MiniMind æ¨¡å‹çš„è¯„ä¼°å’Œäº¤äº’å¼å¯¹è¯åŠŸèƒ½ã€‚
æ”¯æŒå¤šç§æ¨¡å‹æƒé‡ã€LoRA é€‚é…å™¨å’Œç”Ÿæˆå‚æ•°é…ç½®ã€‚

ä¸»è¦åŠŸèƒ½:
1. åŠ è½½ä¸åŒé˜¶æ®µçš„æ¨¡å‹æƒé‡ï¼ˆpretrain, full_sft, dpo, reason ç­‰ï¼‰
2. æ”¯æŒ LoRA æƒé‡åŠ è½½ï¼ˆèº«ä»½è®¤åŒã€åŒ»ç–—é¢†åŸŸç­‰ï¼‰
3. æ”¯æŒè‡ªåŠ¨æµ‹è¯•å’Œæ‰‹åŠ¨è¾“å…¥ä¸¤ç§æ¨¡å¼
4. æ”¯æŒæµå¼è¾“å‡ºï¼ˆé€å­—æ˜¾ç¤ºï¼‰
5. æ”¯æŒå¤šè½®å¯¹è¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡å†å²ï¼‰
6. æ”¯æŒ RoPE ä½ç½®ç¼–ç å¤–æ¨ï¼ˆå¤„ç†é•¿åºåˆ—ï¼‰

æ¨¡å‹æƒé‡è¯´æ˜:
- pretrain: é¢„è®­ç»ƒæƒé‡ï¼Œåªå­¦ä¹ äº†è¯­è¨€æ¨¡å‹
- full_sft: å…¨å‚æ•°ç›‘ç£å¾®è°ƒæƒé‡ï¼Œå­¦ä¹ äº†å¯¹è¯èƒ½åŠ›
- dpo: DPO å¯¹é½åçš„æƒé‡ï¼Œæ›´ç¬¦åˆäººç±»åå¥½
- reason: æ¨ç†è’¸é¦æƒé‡ï¼Œå…·æœ‰æ€è€ƒèƒ½åŠ›
- grpo/spo: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–åçš„æƒé‡

ä½¿ç”¨æ–¹æ³•:
    # åŸºç¡€å¯¹è¯ï¼ˆä½¿ç”¨ full_sft æƒé‡ï¼‰
    python scripts/eval_llm.py
    
    # ä½¿ç”¨æ¨ç†æ¨¡å‹
    python scripts/eval_llm.py --weight reason
    
    # ä½¿ç”¨ LoRA æƒé‡
    python scripts/eval_llm.py --lora_weight lora_identity
    
    # ä½¿ç”¨ MoE æ¶æ„
    python scripts/eval_llm.py --hidden_size 640 --use_moe 1
    
    # å¯ç”¨ RoPE å¤–æ¨ï¼ˆå¤„ç†é•¿åºåˆ—ï¼‰
    python scripts/eval_llm.py --inference_rope_scaling

äº¤äº’æ¨¡å¼:
- æ¨¡å¼ 0: è‡ªåŠ¨æµ‹è¯•ï¼Œä½¿ç”¨é¢„è®¾çš„æµ‹è¯•é—®é¢˜
- æ¨¡å¼ 1: æ‰‹åŠ¨è¾“å…¥ï¼Œç”¨æˆ·è‡ªç”±æé—®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
import random
import warnings
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import *
from trainer.trainer_utils import setup_seed

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')


def init_model(args):
    """
    åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    
    æ ¹æ® args.load_from å‚æ•°å†³å®šåŠ è½½æ–¹å¼:
    - 'model': åŠ è½½åŸç”Ÿ PyTorch æƒé‡ï¼ˆMiniMind æ ¼å¼ï¼‰
    - å…¶ä»–è·¯å¾„: ä½¿ç”¨ HuggingFace transformers åŠ è½½
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…å«:
            - load_from: æ¨¡å‹åŠ è½½è·¯å¾„
            - hidden_size: éšè—å±‚ç»´åº¦
            - num_hidden_layers: éšè—å±‚æ•°é‡
            - use_moe: æ˜¯å¦ä½¿ç”¨ MoE æ¶æ„
            - inference_rope_scaling: æ˜¯å¦å¯ç”¨ RoPE å¤–æ¨
            - save_dir: æƒé‡ä¿å­˜ç›®å½•
            - weight: æƒé‡åç§°å‰ç¼€
            - lora_weight: LoRA æƒé‡åç§°
            - device: è¿è¡Œè®¾å¤‡
            
    Returns:
        Tuple[model, tokenizer]: åŠ è½½å¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨
        
    æ¨¡å‹åŠ è½½æµç¨‹:
    1. åŠ è½½ tokenizer
    2. æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹
    3. åŠ è½½æ¨¡å‹æƒé‡
    4. å¯é€‰ï¼šåŠ è½½ LoRA æƒé‡
    5. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
    """
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(args.load_from)
    
    if 'model' in args.load_from:
        # ========== åŠ è½½ MiniMind åŸç”Ÿæ ¼å¼ ==========
        # åˆ›å»ºæ¨¡å‹é…ç½®
        model = MiniMindForCausalLM(MiniMindConfig(
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_hidden_layers,
            use_moe=bool(args.use_moe),
            inference_rope_scaling=args.inference_rope_scaling  # RoPE å¤–æ¨é…ç½®
        ))
        
        # æ„å»ºæƒé‡æ–‡ä»¶è·¯å¾„
        moe_suffix = '_moe' if args.use_moe else ''
        # å¤„ç†ç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„
        save_dir = args.save_dir if os.path.isabs(args.save_dir) else f'./{args.save_dir}'
        ckp = f'{save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'
        
        # åŠ è½½æ¨¡å‹æƒé‡
        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)
        
        # åŠ è½½ LoRA æƒé‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.lora_weight != 'None':
            # å…ˆåº”ç”¨ LoRA ç»“æ„
            apply_lora(model)
            # å†åŠ è½½ LoRA æƒé‡
            lora_path = f'{save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth'
            load_lora(model, lora_path)
    else:
        # ========== åŠ è½½ HuggingFace æ ¼å¼ ==========
        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    print(f'MiniMindæ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
    return model.eval().to(args.device), tokenizer


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‚æ•°å¹¶è¿è¡Œå¯¹è¯å¾ªç¯
    
    å·¥ä½œæµç¨‹:
    1. è§£æå‘½ä»¤è¡Œå‚æ•°
    2. åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    3. é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼ˆè‡ªåŠ¨æµ‹è¯•/æ‰‹åŠ¨è¾“å…¥ï¼‰
    4. è¿›å…¥å¯¹è¯å¾ªç¯
    """
    # ========== å‘½ä»¤è¡Œå‚æ•°è§£æ ==========
    parser = argparse.ArgumentParser(description="MiniMindæ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    
    # æ¨¡å‹åŠ è½½å‚æ•°
    parser.add_argument('--load_from', default='model', type=str, 
                        help="æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆmodel=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰")
    parser.add_argument('--save_dir', default='out', type=str, 
                        help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument('--weight', default='full_sft', type=str, 
                        help="æƒé‡åç§°å‰ç¼€ï¼ˆpretrain, full_sft, rlhf, reason, ppo_actor, grpo, spoï¼‰")
    parser.add_argument('--lora_weight', default='None', type=str, 
                        help="LoRAæƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šlora_identity, lora_medicalï¼‰")
    
    # æ¨¡å‹æ¶æ„å‚æ•°
    parser.add_argument('--hidden_size', default=512, type=int, 
                        help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰")
    parser.add_argument('--num_hidden_layers', default=8, type=int, 
                        help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], 
                        help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--inference_rope_scaling', default=False, action='store_true', 
                        help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--max_new_tokens', default=8192, type=int, 
                        help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰")
    parser.add_argument('--temperature', default=0.85, type=float, 
                        help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")
    parser.add_argument('--top_p', default=0.85, type=float, 
                        help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    
    # å¯¹è¯å‚æ•°
    parser.add_argument('--historys', default=0, type=int, 
                        help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰")
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str, 
                        help="è¿è¡Œè®¾å¤‡")
    
    args = parser.parse_args()
    
    # ========== é¢„è®¾æµ‹è¯•é—®é¢˜ ==========
    # è¿™äº›é—®é¢˜è¦†ç›–äº†ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œç”¨äºå¿«é€Ÿè¯„ä¼°æ¨¡å‹èƒ½åŠ›
    prompts = [
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',                              # è‡ªæˆ‘ä»‹ç»
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„',                          # ç§‘å­¦è§£é‡Š
        'è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',      # ä»£ç ç”Ÿæˆ
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',                # çŸ¥è¯†é—®ç­”
        'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨',                # æ¨ç†åˆ¤æ–­
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹',              # å¯¹æ¯”åˆ†æ
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',                          # æ¦‚å¿µè§£é‡Š
        'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ'                           # æ¨èä»»åŠ¡
    ]
    
    # ========== åˆå§‹åŒ– ==========
    conversation = []  # å¯¹è¯å†å²
    model, tokenizer = init_model(args)
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n'))
    
    # åˆ›å»ºæµå¼è¾“å‡ºå™¨ï¼ˆé€å­—æ˜¾ç¤ºç”Ÿæˆç»“æœï¼‰
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    # ========== å¯¹è¯å¾ªç¯ ==========
    # æ ¹æ®æ¨¡å¼é€‰æ‹©é—®é¢˜æ¥æº
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ‘¶: '), '')
    
    for prompt in prompt_iter:
        # è®¾ç½®éšæœºç§å­ï¼ˆç¡®ä¿å¯é‡å¤æ€§ï¼‰
        setup_seed(2026)  # æˆ–ä½¿ç”¨ setup_seed(random.randint(0, 2048)) è·å¾—éšæœºç»“æœ
        
        # è‡ªåŠ¨æµ‹è¯•æ¨¡å¼ä¸‹æ‰“å°é—®é¢˜
        if input_mode == 0: 
            print(f'ğŸ‘¶: {prompt}')
        
        # ç®¡ç†å¯¹è¯å†å²
        # åªä¿ç•™æœ€è¿‘ args.historys è½®å¯¹è¯ï¼ˆæ¯è½®åŒ…å« user å’Œ assistant ä¸¤æ¡æ¶ˆæ¯ï¼‰
        conversation = conversation[-args.historys:] if args.historys else []
        conversation.append({"role": "user", "content": prompt})

        # ========== æ„å»ºè¾“å…¥ ==========
        # ä½¿ç”¨ chat_template æ ¼å¼åŒ–å¯¹è¯
        templates = {
            "conversation": conversation, 
            "tokenize": False, 
            "add_generation_prompt": True
        }
        
        # æ¨ç†æ¨¡å‹ç‰¹æ®Šå¤„ç†ï¼šå¯ç”¨æ€è€ƒæ¨¡å¼
        if args.weight == 'reason': 
            templates["enable_thinking"] = True
        
        # æ ¹æ®æƒé‡ç±»å‹é€‰æ‹©è¾“å…¥æ ¼å¼
        if args.weight != 'pretrain':
            # SFT/DPO/Reason ç­‰æ¨¡å‹ä½¿ç”¨ chat_template
            inputs = tokenizer.apply_chat_template(**templates)
        else:
            # é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨ç®€å•æ ¼å¼
            inputs = tokenizer.bos_token + prompt
        
        # Tokenize è¾“å…¥
        inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)

        # ========== ç”Ÿæˆå›å¤ ==========
        print('ğŸ¤–ï¸: ', end='')
        
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
        generated_ids = model.generate(
            inputs=inputs["input_ids"],           # è¾“å…¥ token ID
            attention_mask=inputs["attention_mask"],  # æ³¨æ„åŠ›æ©ç 
            max_new_tokens=args.max_new_tokens,   # æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample=True,                        # ä½¿ç”¨é‡‡æ ·
            streamer=streamer,                     # æµå¼è¾“å‡º
            pad_token_id=tokenizer.pad_token_id,  # å¡«å…… token ID
            eos_token_id=tokenizer.eos_token_id,  # ç»“æŸ token ID
            top_p=args.top_p,                      # Top-p é‡‡æ ·
            temperature=args.temperature,          # æ¸©åº¦å‚æ•°
            repetition_penalty=1.0                 # é‡å¤æƒ©ç½šï¼ˆ1.0 è¡¨ç¤ºä¸æƒ©ç½šï¼‰
        )
        
        # è§£ç ç”Ÿæˆçš„å›å¤ï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
        response = tokenizer.decode(
            generated_ids[0][len(inputs["input_ids"][0]):], 
            skip_special_tokens=True
        )
        
        # å°†å›å¤æ·»åŠ åˆ°å¯¹è¯å†å²
        conversation.append({"role": "assistant", "content": response})
        
        print('\n\n')


if __name__ == "__main__":
    main()

