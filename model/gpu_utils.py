"""
GPU å…¼å®¹æ€§å·¥å…·

æœ¬æ¨¡å—æä¾› GPU æ£€æµ‹å’Œå…¼å®¹æ€§é…ç½®åŠŸèƒ½ï¼Œç‰¹åˆ«æ”¯æŒæœ€æ–°çš„ GPU æ¶æ„ã€‚

ä¸»è¦åŠŸèƒ½:
1. æ£€æµ‹ GPU è®¡ç®—èƒ½åŠ›ï¼ˆCompute Capabilityï¼‰
2. è‡ªåŠ¨é…ç½® Flash Attention æ”¯æŒ
3. æ¨èæœ€ä½³æ•°æ®ç±»å‹ï¼ˆdtypeï¼‰
4. ç¡®ä¿æ¨¡å‹é…ç½®ä¸ GPU å…¼å®¹

æ”¯æŒçš„ GPU æ¶æ„:
- Blackwell (sm_120): RTX 5090, 5080 ç­‰
- Hopper (sm_90): H100, H200 ç­‰
- Ada Lovelace (sm_89): RTX 4090, 4080, 4070 ç­‰
- Ampere (sm_80/86): RTX 3090, 3080, A100 ç­‰
- Turing (sm_75): RTX 2080, T4 ç­‰
- Volta (sm_70): V100 ç­‰

ä½¿ç”¨æ–¹æ³•:
    from model.gpu_utils import ensure_gpu_compatibility
    
    config = MiniMindConfig(...)
    config = ensure_gpu_compatibility(config)  # è‡ªåŠ¨é…ç½®
"""

import torch


def get_gpu_compute_capability():
    """
    è·å–å½“å‰ GPU çš„è®¡ç®—èƒ½åŠ›ï¼ˆCompute Capabilityï¼‰
    
    è®¡ç®—èƒ½åŠ›æ˜¯ NVIDIA GPU çš„ç‰ˆæœ¬æ ‡è¯†ï¼Œæ ¼å¼ä¸º (major, minor)ã€‚
    ä¸åŒçš„è®¡ç®—èƒ½åŠ›å¯¹åº”ä¸åŒçš„ GPU æ¶æ„å’ŒåŠŸèƒ½æ”¯æŒã€‚
    
    è®¡ç®—èƒ½åŠ›å¯¹ç…§è¡¨:
    - 12.0: Blackwell (RTX 5090, 5080)
    - 9.0: Hopper (H100, H200)
    - 8.9: Ada Lovelace (RTX 4090, 4080, 4070)
    - 8.6: Ampere æ¶ˆè´¹çº§ (RTX 3090, 3080, 3070)
    - 8.0: Ampere æ•°æ®ä¸­å¿ƒ (A100, A10)
    - 7.5: Turing (RTX 2080, T4)
    - 7.0: Volta (V100)
    
    Returns:
        Tuple[int, int]: (major, minor) è®¡ç®—èƒ½åŠ›ç‰ˆæœ¬
                        å¦‚æœæ²¡æœ‰ GPU æˆ–æ£€æµ‹å¤±è´¥ï¼Œè¿”å› (0, 0)
    
    Example:
        >>> major, minor = get_gpu_compute_capability()
        >>> print(f"GPU è®¡ç®—èƒ½åŠ›: {major}.{minor}")
        GPU è®¡ç®—èƒ½åŠ›: 8.9
    """
    if not torch.cuda.is_available():
        return (0, 0)
    
    try:
        # è·å–å½“å‰è®¾å¤‡çš„è®¡ç®—èƒ½åŠ›
        device = torch.cuda.current_device()
        major, minor = torch.cuda.get_device_capability(device)
        return (major, minor)
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è·å– GPU è®¡ç®—èƒ½åŠ›: {e}")
        return (0, 0)


def check_flash_attention_support():
    """
    æ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦æ”¯æŒ Flash Attention
    
    Flash Attention æ˜¯ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—å®ç°ï¼Œå¯ä»¥:
    1. å‡å°‘å†…å­˜ä½¿ç”¨ï¼ˆä» O(nÂ²) é™ä½åˆ° O(n)ï¼‰
    2. åŠ é€Ÿè®¡ç®—ï¼ˆé€šè¿‡å‡å°‘å†…å­˜è®¿é—®ï¼‰
    3. æ”¯æŒæ›´é•¿çš„åºåˆ—
    
    æ”¯æŒæ¡ä»¶:
    1. PyTorch >= 2.0ï¼ˆåŒ…å« scaled_dot_product_attentionï¼‰
    2. CUDA å¯ç”¨
    3. GPU è®¡ç®—èƒ½åŠ› >= 7.0ï¼ˆVolta åŠä»¥ä¸Šï¼‰
    
    Returns:
        bool: æ˜¯å¦æ”¯æŒ Flash Attention
        
    Example:
        >>> if check_flash_attention_support():
        ...     print("å¯ä»¥ä½¿ç”¨ Flash Attention")
    """
    # æ£€æŸ¥ PyTorch ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ SDPA
    if not hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
        return False
    
    # æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        return False
    
    # æ£€æŸ¥ GPU è®¡ç®—èƒ½åŠ›
    major, minor = get_gpu_compute_capability()
    
    # Flash Attention éœ€è¦ Volta (7.0) åŠä»¥ä¸Šæ¶æ„
    if major < 7:
        return False
    
    return True


def get_recommended_dtype():
    """
    æ ¹æ® GPU èƒ½åŠ›æ¨èæœ€ä½³æ•°æ®ç±»å‹
    
    ä¸åŒçš„ GPU æ¶æ„å¯¹ä¸åŒæ•°æ®ç±»å‹æœ‰ä¸åŒçš„æ”¯æŒå’Œæ€§èƒ½:
    
    - bfloat16: 
      - ä¼˜ç‚¹: åŠ¨æ€èŒƒå›´å¤§ï¼Œè®­ç»ƒç¨³å®š
      - æ”¯æŒ: Ampere (8.0) åŠä»¥ä¸Š
      - æ¨èç”¨äº: è®­ç»ƒå’Œæ¨ç†
      
    - float16:
      - ä¼˜ç‚¹: å¹¿æ³›æ”¯æŒï¼Œå†…å­˜æ•ˆç‡é«˜
      - æ”¯æŒ: æ‰€æœ‰ç°ä»£ GPU
      - æ³¨æ„: å¯èƒ½éœ€è¦æŸå¤±ç¼©æ”¾
      
    - float32:
      - ä¼˜ç‚¹: æœ€é«˜ç²¾åº¦
      - æ”¯æŒ: æ‰€æœ‰ GPU
      - ç¼ºç‚¹: å†…å­˜ä½¿ç”¨å¤šï¼Œé€Ÿåº¦æ…¢
    
    Returns:
        str: æ¨èçš„æ•°æ®ç±»å‹ ('bfloat16', 'float16', æˆ– 'float32')
        
    Example:
        >>> dtype = get_recommended_dtype()
        >>> print(f"æ¨èä½¿ç”¨: {dtype}")
    """
    if not torch.cuda.is_available():
        return 'float32'
    
    # æ£€æŸ¥ bfloat16 æ”¯æŒ
    if torch.cuda.is_bf16_supported():
        return 'bfloat16'
    
    # æ£€æŸ¥ GPU è®¡ç®—èƒ½åŠ›
    major, minor = get_gpu_compute_capability()
    
    # Ampere (8.0) åŠä»¥ä¸Šæ”¯æŒ bfloat16
    if major >= 8:
        return 'bfloat16'
    
    # å…¶ä»– GPU ä½¿ç”¨ float16
    if major >= 7:
        return 'float16'
    
    # è¾ƒè€çš„ GPU ä½¿ç”¨ float32
    return 'float32'


def print_gpu_info():
    """
    æ‰“å°è¯¦ç»†çš„ GPU ä¿¡æ¯
    
    è¾“å‡ºåŒ…æ‹¬:
    - GPU åç§°å’Œæ•°é‡
    - è®¡ç®—èƒ½åŠ›
    - æ˜¾å­˜å¤§å°
    - CUDA ç‰ˆæœ¬
    - Flash Attention æ”¯æŒçŠ¶æ€
    - æ¨èçš„æ•°æ®ç±»å‹
    
    Example:
        >>> print_gpu_info()
        ============================================================
        ğŸ” GPU ä¿¡æ¯
        ============================================================
        ğŸ“Œ GPU æ•°é‡: 1
        ğŸ“Œ GPU 0: NVIDIA GeForce RTX 4090
           - è®¡ç®—èƒ½åŠ›: 8.9
           - æ˜¾å­˜: 24.0 GB
        ğŸ“Œ CUDA ç‰ˆæœ¬: 12.1
        ğŸ“Œ Flash Attention: âœ… æ”¯æŒ
        ğŸ“Œ æ¨è dtype: bfloat16
        ============================================================
    """
    print("\n" + "="*60)
    print("ğŸ” GPU ä¿¡æ¯")
    print("="*60)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨")
        print("="*60)
        return
    
    # GPU æ•°é‡
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ“Œ GPU æ•°é‡: {gpu_count}")
    
    # æ¯ä¸ª GPU çš„è¯¦ç»†ä¿¡æ¯
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / (1024**3)
        major, minor = props.major, props.minor
        
        print(f"ğŸ“Œ GPU {i}: {props.name}")
        print(f"   - è®¡ç®—èƒ½åŠ›: {major}.{minor}")
        print(f"   - æ˜¾å­˜: {memory_gb:.1f} GB")
    
    # CUDA ç‰ˆæœ¬
    print(f"ğŸ“Œ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    
    # Flash Attention æ”¯æŒ
    flash_support = check_flash_attention_support()
    flash_status = "âœ… æ”¯æŒ" if flash_support else "âŒ ä¸æ”¯æŒ"
    print(f"ğŸ“Œ Flash Attention: {flash_status}")
    
    # æ¨èçš„æ•°æ®ç±»å‹
    recommended_dtype = get_recommended_dtype()
    print(f"ğŸ“Œ æ¨è dtype: {recommended_dtype}")
    
    print("="*60)


def ensure_gpu_compatibility(config):
    """
    ç¡®ä¿æ¨¡å‹é…ç½®ä¸å½“å‰ GPU å…¼å®¹
    
    æ ¹æ® GPU èƒ½åŠ›è‡ªåŠ¨è°ƒæ•´é…ç½®:
    1. æ£€æµ‹ Flash Attention æ”¯æŒå¹¶é…ç½®
    2. å¯¹äºä¸æ”¯æŒçš„ GPUï¼Œç¦ç”¨æŸäº›é«˜çº§ç‰¹æ€§
    3. æ‰“å°å…¼å®¹æ€§ä¿¡æ¯
    
    Args:
        config: MiniMindConfig é…ç½®å¯¹è±¡
        
    Returns:
        config: è°ƒæ•´åçš„é…ç½®å¯¹è±¡
        
    Example:
        >>> config = MiniMindConfig(flash_attn=True)
        >>> config = ensure_gpu_compatibility(config)
        >>> # å¦‚æœ GPU ä¸æ”¯æŒ Flash Attentionï¼Œä¼šè‡ªåŠ¨ç¦ç”¨
    """
    if not torch.cuda.is_available():
        # CPU æ¨¡å¼: ç¦ç”¨ Flash Attention
        if hasattr(config, 'flash_attn'):
            config.flash_attn = False
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU æ¨¡å¼")
        return config
    
    # è·å– GPU ä¿¡æ¯
    major, minor = get_gpu_compute_capability()
    
    # æ£€æŸ¥ Flash Attention æ”¯æŒ
    flash_support = check_flash_attention_support()
    
    if hasattr(config, 'flash_attn'):
        if config.flash_attn and not flash_support:
            print(f"âš ï¸  GPU (sm_{major}{minor}) ä¸æ”¯æŒ Flash Attentionï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
            config.flash_attn = False
        elif config.flash_attn and flash_support:
            print(f"âœ… Flash Attention å·²å¯ç”¨ (GPU: sm_{major}{minor})")
    
    # ç‰¹æ®Šæ¶æ„å¤„ç†
    if major >= 12:
        # Blackwell æ¶æ„ (sm_120)
        print(f"ğŸš€ æ£€æµ‹åˆ° Blackwell æ¶æ„ (sm_{major}{minor})")
        print("   å»ºè®®ä½¿ç”¨ bfloat16 ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    elif major >= 9:
        # Hopper æ¶æ„ (sm_90)
        print(f"ğŸš€ æ£€æµ‹åˆ° Hopper æ¶æ„ (sm_{major}{minor})")
    elif major >= 8:
        # Ampere/Ada æ¶æ„
        if minor >= 9:
            print(f"âœ… æ£€æµ‹åˆ° Ada Lovelace æ¶æ„ (sm_{major}{minor})")
        else:
            print(f"âœ… æ£€æµ‹åˆ° Ampere æ¶æ„ (sm_{major}{minor})")
    
    return config


def get_optimal_batch_size(hidden_size, seq_len, gpu_memory_gb=None):
    """
    æ ¹æ® GPU æ˜¾å­˜ä¼°ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    
    è¿™æ˜¯ä¸€ä¸ªç²—ç•¥ä¼°ç®—ï¼Œå®é™…æœ€ä¼˜å€¼å¯èƒ½éœ€è¦é€šè¿‡å®éªŒç¡®å®šã€‚
    
    ä¼°ç®—å…¬å¼ï¼ˆç®€åŒ–ï¼‰:
    memory_per_sample â‰ˆ 4 * hidden_size * seq_len * num_layers * 2 (å‰å‘+åå‘)
    
    Args:
        hidden_size (int): æ¨¡å‹éšè—å±‚ç»´åº¦
        seq_len (int): åºåˆ—é•¿åº¦
        gpu_memory_gb (float): GPU æ˜¾å­˜å¤§å°ï¼ˆGBï¼‰ï¼ŒNone åˆ™è‡ªåŠ¨æ£€æµ‹
        
    Returns:
        int: æ¨èçš„æ‰¹æ¬¡å¤§å°
        
    Example:
        >>> batch_size = get_optimal_batch_size(512, 512)
        >>> print(f"æ¨èæ‰¹æ¬¡å¤§å°: {batch_size}")
    """
    if gpu_memory_gb is None:
        if torch.cuda.is_available():
            props = torch.cuda.get_device_properties(0)
            gpu_memory_gb = props.total_memory / (1024**3)
        else:
            gpu_memory_gb = 8  # é»˜è®¤å‡è®¾ 8GB
    
    # é¢„ç•™ä¸€éƒ¨åˆ†æ˜¾å­˜ç»™ç³»ç»Ÿå’Œå…¶ä»–å¼€é”€
    available_memory_gb = gpu_memory_gb * 0.7
    
    # ç²—ç•¥ä¼°ç®—æ¯ä¸ªæ ·æœ¬çš„å†…å­˜ä½¿ç”¨ï¼ˆå­—èŠ‚ï¼‰
    # è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€åŒ–çš„ä¼°ç®—
    bytes_per_sample = hidden_size * seq_len * 4 * 8  # å‡è®¾ 8 å±‚ï¼Œfloat32
    
    # è®¡ç®—æ‰¹æ¬¡å¤§å°
    batch_size = int(available_memory_gb * 1024**3 / bytes_per_sample)
    
    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    batch_size = max(1, min(batch_size, 128))
    
    # å¯¹é½åˆ° 2 çš„å¹‚æ¬¡ï¼ˆå¯é€‰ï¼Œæœ‰æ—¶èƒ½æé«˜æ•ˆç‡ï¼‰
    # batch_size = 2 ** int(math.log2(batch_size))
    
    return batch_size
