"""
MiniMind æ¨¡å‹å®ç°

æœ¬æ–‡ä»¶å®ç°äº† MiniMind è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæ¶æ„ï¼ŒåŒ…æ‹¬ï¼š
1. MiniMindConfig - æ¨¡å‹é…ç½®ç±»ï¼Œå®šä¹‰æ‰€æœ‰è¶…å‚æ•°
2. RMSNorm - Root Mean Square Layer Normalization
3. Attention - å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆæ”¯æŒ GQA å’Œ Flash Attentionï¼‰
4. FeedForward - å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆSwiGLU æ¿€æ´»ï¼‰
5. MoEGate - æ··åˆä¸“å®¶é—¨æ§æœºåˆ¶
6. MOEFeedForward - æ··åˆä¸“å®¶å‰é¦ˆç½‘ç»œ
7. MiniMindBlock - Transformer è§£ç å™¨å±‚
8. MiniMindModel - å®Œæ•´çš„ Transformer æ¨¡å‹
9. MiniMindForCausalLM - å› æœè¯­è¨€æ¨¡å‹ï¼ˆç”¨äºæ–‡æœ¬ç”Ÿæˆï¼‰

æ¨¡å‹æ¶æ„ç‰¹ç‚¹ï¼š
- é‡‡ç”¨ Pre-Norm ç»“æ„ï¼ˆLayerNorm åœ¨æ³¨æ„åŠ›å’Œ FFN ä¹‹å‰ï¼‰
- ä½¿ç”¨ RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰è¿›è¡Œä½ç½®ç¼–ç 
- æ”¯æŒ GQAï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰ä»¥å‡å°‘ KV ç¼“å­˜
- æ”¯æŒ MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¶æ„ä»¥å¢åŠ æ¨¡å‹å®¹é‡
- æ”¯æŒ Flash Attention åŠ é€Ÿæ¨ç†
- æ”¯æŒ YaRN ä½ç½®ç¼–ç å¤–æ¨ä»¥å¤„ç†é•¿åºåˆ—
"""

# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

from transformers import PretrainedConfig


class MiniMindConfig(PretrainedConfig):
    """
    MiniMind æ¨¡å‹é…ç½®ç±»
    
    ç»§æ‰¿è‡ª HuggingFace çš„ PretrainedConfigï¼Œç”¨äºå®šä¹‰æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°ã€‚
    è¿™äº›å‚æ•°æ§åˆ¶æ¨¡å‹çš„æ¶æ„ã€å¤§å°å’Œè¡Œä¸ºã€‚
    
    Attributes:
        model_type (str): æ¨¡å‹ç±»å‹æ ‡è¯†ç¬¦ï¼Œç”¨äº HuggingFace è‡ªåŠ¨åŠ è½½
        
    åŸºç¡€æ¶æ„å‚æ•°:
        dropout (float): Dropout æ¦‚ç‡ï¼Œç”¨äºæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
        bos_token_id (int): åºåˆ—å¼€å§‹æ ‡è®°çš„ ID
        eos_token_id (int): åºåˆ—ç»“æŸæ ‡è®°çš„ ID
        hidden_act (str): éšè—å±‚æ¿€æ´»å‡½æ•°ç±»å‹ï¼ˆé»˜è®¤ 'silu'ï¼Œå³ SwiGLUï¼‰
        hidden_size (int): éšè—å±‚ç»´åº¦ï¼Œå†³å®šæ¨¡å‹çš„å®½åº¦
        intermediate_size (int): FFN ä¸­é—´å±‚ç»´åº¦ï¼Œé€šå¸¸ä¸º hidden_size çš„ 8/3 å€
        max_position_embeddings (int): æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦
        num_attention_heads (int): æ³¨æ„åŠ›å¤´æ•°é‡
        num_hidden_layers (int): Transformer å±‚æ•°ï¼Œå†³å®šæ¨¡å‹çš„æ·±åº¦
        num_key_value_heads (int): KV å¤´æ•°é‡ï¼ˆç”¨äº GQAï¼Œå‡å°‘ KV ç¼“å­˜ï¼‰
        vocab_size (int): è¯è¡¨å¤§å°
        rms_norm_eps (float): RMSNorm çš„ epsilon å€¼ï¼Œé˜²æ­¢é™¤é›¶
        rope_theta (float): RoPE ä½ç½®ç¼–ç çš„åŸºç¡€é¢‘ç‡
        inference_rope_scaling (bool): æ˜¯å¦å¯ç”¨æ¨ç†æ—¶çš„ RoPE ç¼©æ”¾ï¼ˆYaRNï¼‰
        flash_attn (bool): æ˜¯å¦ä½¿ç”¨ Flash Attention åŠ é€Ÿ
        
    MoEï¼ˆæ··åˆä¸“å®¶ï¼‰å‚æ•°:
        use_moe (bool): æ˜¯å¦ä½¿ç”¨ MoE æ¶æ„
        num_experts_per_tok (int): æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°é‡
        n_routed_experts (int): å¯è·¯ç”±ä¸“å®¶çš„æ€»æ•°é‡
        n_shared_experts (int): å…±äº«ä¸“å®¶æ•°é‡ï¼ˆæ‰€æœ‰ token éƒ½ä¼šä½¿ç”¨ï¼‰
        scoring_func (str): ä¸“å®¶é€‰æ‹©çš„è¯„åˆ†å‡½æ•°
        aux_loss_alpha (float): è¾…åŠ©æŸå¤±çš„æƒé‡ç³»æ•°
        seq_aux (bool): æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤±
        norm_topk_prob (bool): æ˜¯å¦å½’ä¸€åŒ– top-k æ¦‚ç‡
    """
    model_type = "minimind"  # HuggingFace æ¨¡å‹ç±»å‹æ ‡è¯†

    def __init__(
            self,
            dropout: float = 0.0,                    # Dropout æ¦‚ç‡
            bos_token_id: int = 1,                   # åºåˆ—å¼€å§‹æ ‡è®° ID
            eos_token_id: int = 2,                   # åºåˆ—ç»“æŸæ ‡è®° ID
            hidden_act: str = 'silu',                # æ¿€æ´»å‡½æ•°ç±»å‹
            hidden_size: int = 512,                  # éšè—å±‚ç»´åº¦
            intermediate_size: int = None,          # FFN ä¸­é—´å±‚ç»´åº¦ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰
            max_position_embeddings: int = 32768,   # æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦
            num_attention_heads: int = 8,            # æ³¨æ„åŠ›å¤´æ•°
            num_hidden_layers: int = 8,              # Transformer å±‚æ•°
            num_key_value_heads: int = 2,            # KV å¤´æ•°ï¼ˆGQAï¼‰
            vocab_size: int = 6400,                  # è¯è¡¨å¤§å°
            rms_norm_eps: float = 1e-05,            # RMSNorm epsilon
            rope_theta: int = 1000000.0,            # RoPE åŸºç¡€é¢‘ç‡
            inference_rope_scaling: bool = False,   # æ˜¯å¦å¯ç”¨ RoPE ç¼©æ”¾
            flash_attn: bool = True,                 # æ˜¯å¦ä½¿ç”¨ Flash Attention
            ####################################################
            # ä»¥ä¸‹æ˜¯ MoEï¼ˆæ··åˆä¸“å®¶ï¼‰çš„ç‰¹å®šé…ç½®
            # å½“ use_moe ä¸º False æ—¶ï¼Œä»¥ä¸‹é…ç½®æ— æ•ˆ
            ####################################################
            use_moe: bool = False,                   # æ˜¯å¦ä½¿ç”¨ MoE
            num_experts_per_tok: int = 2,            # æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°
            n_routed_experts: int = 4,               # å¯è·¯ç”±ä¸“å®¶æ€»æ•°
            n_shared_experts: int = 1,               # å…±äº«ä¸“å®¶æ•°é‡
            scoring_func: str = 'softmax',           # è¯„åˆ†å‡½æ•°
            aux_loss_alpha: float = 0.1,             # è¾…åŠ©æŸå¤±æƒé‡
            seq_aux: bool = True,                    # åºåˆ—çº§è¾…åŠ©æŸå¤±
            norm_topk_prob: bool = True,             # å½’ä¸€åŒ– top-k æ¦‚ç‡
            **kwargs
    ):
        """
        åˆå§‹åŒ– MiniMind é…ç½®
        
        Args:
            dropout: Dropout æ¦‚ç‡ï¼ŒèŒƒå›´ [0, 1]
            bos_token_id: åºåˆ—å¼€å§‹æ ‡è®°çš„ token ID
            eos_token_id: åºåˆ—ç»“æŸæ ‡è®°çš„ token ID
            hidden_act: æ¿€æ´»å‡½æ•°åç§°ï¼Œæ”¯æŒ 'silu', 'gelu', 'relu' ç­‰
            hidden_size: æ¨¡å‹éšè—å±‚ç»´åº¦ï¼Œå¸¸è§å€¼ï¼š512, 768, 1024
            intermediate_size: FFN ä¸­é—´å±‚ç»´åº¦ï¼ŒNone æ—¶è‡ªåŠ¨è®¡ç®—ä¸º hidden_size * 8/3
            max_position_embeddings: æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦
            num_attention_heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            num_hidden_layers: Transformer è§£ç å™¨å±‚æ•°
            num_key_value_heads: GQA ä¸­çš„ KV å¤´æ•°ï¼Œå°äº num_attention_heads å¯å‡å°‘å†…å­˜
            vocab_size: è¯è¡¨å¤§å°ï¼Œéœ€ä¸ tokenizer åŒ¹é…
            rms_norm_eps: RMSNorm ä¸­é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
            rope_theta: RoPE ä½ç½®ç¼–ç çš„åŸºç¡€é¢‘ç‡ï¼Œå½±å“ä½ç½®ç¼–ç çš„å‘¨æœŸ
            inference_rope_scaling: å¯ç”¨ YaRN ä½ç½®ç¼–ç å¤–æ¨ï¼Œæ”¯æŒæ›´é•¿åºåˆ—
            flash_attn: ä½¿ç”¨ Flash Attention 2 åŠ é€Ÿï¼Œéœ€è¦ PyTorch >= 2.0
            use_moe: å¯ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œå¢åŠ æ¨¡å‹å®¹é‡ä½†ä¿æŒè®¡ç®—é‡
            num_experts_per_tok: æ¯ä¸ª token è·¯ç”±åˆ°çš„ä¸“å®¶æ•°é‡
            n_routed_experts: å¯è·¯ç”±ä¸“å®¶çš„æ€»æ•°é‡
            n_shared_experts: å…±äº«ä¸“å®¶æ•°é‡ï¼Œæ‰€æœ‰ token éƒ½ä¼šç»è¿‡
            scoring_func: ä¸“å®¶é€‰æ‹©çš„è¯„åˆ†å‡½æ•°ï¼Œç›®å‰æ”¯æŒ 'softmax'
            aux_loss_alpha: è¾…åŠ©æŸå¤±æƒé‡ï¼Œç”¨äºå¹³è¡¡ä¸“å®¶è´Ÿè½½
            seq_aux: æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤±
            norm_topk_prob: æ˜¯å¦å¯¹é€‰ä¸­ä¸“å®¶çš„æ¦‚ç‡è¿›è¡Œå½’ä¸€åŒ–
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»çš„å…¶ä»–å‚æ•°
        """
        super().__init__(**kwargs)
        
        # åŸºç¡€æ¶æ„å‚æ•°
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        
        # YaRN ä½ç½®ç¼–ç å¤–æ¨é…ç½®
        # å½“å¯ç”¨ inference_rope_scaling æ—¶ï¼Œä½¿ç”¨ YaRN æ–¹æ³•æ‰©å±•ä½ç½®ç¼–ç 
        # å¤–æ¨é•¿åº¦ = factor * original_max_position_embeddings
        self.rope_scaling = {
            "beta_fast": 4,                          # å¿«é€Ÿè¡°å‡å› å­
            "beta_slow": 1,                          # æ…¢é€Ÿè¡°å‡å› å­
            "factor": 4,                             # å¤–æ¨å€æ•°
            "original_max_position_embeddings": 2048, # åŸå§‹æœ€å¤§ä½ç½®
            "type": "yarn"                           # å¤–æ¨ç±»å‹
        } if self.inference_rope_scaling else None
        
        self.flash_attn = flash_attn
        
        ####################################################
        # MoEï¼ˆæ··åˆä¸“å®¶ï¼‰é…ç½®
        # å½“ use_moe ä¸º False æ—¶ï¼Œä»¥ä¸‹é…ç½®æ— æ•ˆ
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts        # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts        # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func                # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º 'softmax'
        self.aux_loss_alpha = aux_loss_alpha            # è¾…åŠ©æŸå¤±çš„ alpha å‚æ•°
        self.seq_aux = seq_aux                          # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob            # æ˜¯å¦æ ‡å‡†åŒ– top-k æ¦‚ç‡


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

import math
import torch
import torch.nn.init as init
import torch.nn.functional as F
from torch import nn
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast


class RMSNorm(torch.nn.Module):
    """
    Root Mean Square Layer Normalization (RMSNorm)
    
    RMSNorm æ˜¯ LayerNorm çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œåªè¿›è¡Œç¼©æ”¾è€Œä¸è¿›è¡Œä¸­å¿ƒåŒ–ã€‚
    ç›¸æ¯” LayerNormï¼ŒRMSNorm è®¡ç®—æ›´é«˜æ•ˆï¼Œä¸”åœ¨å®è·µä¸­æ•ˆæœç›¸å½“ã€‚
    
    å…¬å¼: output = x / sqrt(mean(x^2) + eps) * weight
    
    ä¸ LayerNorm çš„åŒºåˆ«:
    - LayerNorm: (x - mean) / sqrt(var + eps) * gamma + beta
    - RMSNorm: x / sqrt(mean(x^2) + eps) * weight
    
    ä¼˜ç‚¹:
    1. è®¡ç®—æ›´ç®€å•ï¼Œä¸éœ€è¦è®¡ç®—å‡å€¼
    2. å‚æ•°æ›´å°‘ï¼Œåªæœ‰ weight æ²¡æœ‰ bias
    3. åœ¨ LLM ä¸­æ•ˆæœä¸ LayerNorm ç›¸å½“
    
    Attributes:
        eps (float): é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
        weight (nn.Parameter): å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°
    """
    
    def __init__(self, dim: int, eps: float = 1e-5):
        """
        åˆå§‹åŒ– RMSNorm
        
        Args:
            dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            eps: é˜²æ­¢é™¤é›¶çš„ epsilon å€¼
        """
        super().__init__()
        self.eps = eps
        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼Œåˆå§‹åŒ–ä¸ºå…¨ 1
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        è®¡ç®— RMS å½’ä¸€åŒ–
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., dim)
            
        Returns:
            å½’ä¸€åŒ–åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        # è®¡ç®— x^2 çš„å‡å€¼ï¼Œç„¶åå–å€’æ•°å¹³æ–¹æ ¹
        # rsqrt = 1 / sqrt(x)ï¼Œæ¯” 1 / sqrt(x) æ›´é«˜æ•ˆ
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, dim)
            
        Returns:
            å½’ä¸€åŒ–åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        # å…ˆè½¬ä¸º float32 è¿›è¡Œå½’ä¸€åŒ–è®¡ç®—ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
        # ç„¶åè½¬å›åŸå§‹æ•°æ®ç±»å‹ï¼Œæœ€åä¹˜ä»¥å¯å­¦ä¹ çš„æƒé‡
        return self.weight * self._norm(x.float()).type_as(x)


def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), rope_base: float = 1e6,
                         rope_scaling: Optional[dict] = None):
    """
    é¢„è®¡ç®— RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰çš„é¢‘ç‡
    
    RoPE æ˜¯ä¸€ç§ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ³•ï¼Œé€šè¿‡æ—‹è½¬å‘é‡æ¥ç¼–ç ä½ç½®ä¿¡æ¯ã€‚
    å®ƒçš„ä¼˜ç‚¹æ˜¯å¯ä»¥è‡ªç„¶åœ°å¤„ç†ç›¸å¯¹ä½ç½®ï¼Œä¸”æ”¯æŒå¤–æ¨åˆ°æ›´é•¿çš„åºåˆ—ã€‚
    
    RoPE å…¬å¼:
    - å¯¹äºä½ç½® m å’Œç»´åº¦ iï¼Œæ—‹è½¬è§’åº¦ Î¸_i = m * base^(-2i/d)
    - åº”ç”¨æ—‹è½¬: (q_2i, q_2i+1) -> (q_2i*cos(Î¸) - q_2i+1*sin(Î¸), q_2i*sin(Î¸) + q_2i+1*cos(Î¸))
    
    YaRN å¤–æ¨:
    å½“ rope_scaling ä¸ä¸º None æ—¶ï¼Œä½¿ç”¨ YaRN æ–¹æ³•è¿›è¡Œä½ç½®ç¼–ç å¤–æ¨ã€‚
    YaRN é€šè¿‡è°ƒæ•´ä¸åŒé¢‘ç‡åˆ†é‡çš„ç¼©æ”¾å› å­ï¼Œå®ç°æ›´å¥½çš„é•¿åº¦å¤–æ¨ã€‚
    
    Args:
        dim: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
        end: é¢„è®¡ç®—çš„æœ€å¤§ä½ç½®æ•°
        rope_base: RoPE çš„åŸºç¡€é¢‘ç‡ï¼ˆthetaï¼‰
        rope_scaling: YaRN å¤–æ¨é…ç½®ï¼ŒåŒ…å« factor, beta_fast, beta_slow ç­‰å‚æ•°
        
    Returns:
        Tuple[Tensor, Tensor]: (freqs_cos, freqs_sin)
            - freqs_cos: ä½™å¼¦é¢‘ç‡ï¼Œå½¢çŠ¶ä¸º (end, dim)
            - freqs_sin: æ­£å¼¦é¢‘ç‡ï¼Œå½¢çŠ¶ä¸º (end, dim)
    """
    # è®¡ç®—åŸºç¡€é¢‘ç‡: 1 / (base^(2i/d))ï¼Œå…¶ä¸­ i = 0, 1, ..., d/2-1
    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    
    # å¦‚æœå¯ç”¨ YaRN å¤–æ¨
    if rope_scaling is not None:
        # è·å– YaRN å‚æ•°
        orig_max, factor, beta_fast, beta_slow = (
            rope_scaling.get("original_max_position_embeddings", 2048),  # åŸå§‹æœ€å¤§ä½ç½®
            rope_scaling.get("factor", 4),                               # å¤–æ¨å€æ•°
            rope_scaling.get("beta_fast", 4.0),                          # å¿«é€Ÿè¡°å‡å› å­
            rope_scaling.get("beta_slow", 1.0)                           # æ…¢é€Ÿè¡°å‡å› å­
        )
        
        # åªæœ‰å½“ç›®æ ‡é•¿åº¦è¶…è¿‡åŸå§‹æœ€å¤§é•¿åº¦æ—¶æ‰è¿›è¡Œå¤–æ¨
        if end / orig_max > 1.0:
            # æ‰¾åˆ°éœ€è¦è°ƒæ•´çš„ç»´åº¦è¾¹ç•Œ
            # å¯¹äºå‘¨æœŸå¤§äºåŸå§‹æœ€å¤§ä½ç½®çš„é¢‘ç‡åˆ†é‡ï¼Œéœ€è¦è¿›è¡Œç¼©æ”¾
            corr_dim = next((i for i in range(dim // 2) if 2 * math.pi / freqs[i] > orig_max), dim // 2)
            
            # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ’å€¼æƒé‡
            power = torch.arange(0, dim // 2, device=freqs.device).float() / max(dim // 2 - 1, 1)
            beta = beta_slow + (beta_fast - beta_slow) * power
            
            # YaRN æ ‡å‡†å…¬å¼: Î» = (Î²Â·Î± - Î² + 1) / (Î²Â·Î±)
            # å…¶ä¸­ Î± = factorï¼ŒÎ² æ˜¯æ’å€¼æƒé‡
            scale = torch.where(
                torch.arange(dim // 2, device=freqs.device) < corr_dim,
                (beta * factor - beta + 1) / (beta * factor),  # éœ€è¦ç¼©æ”¾çš„ç»´åº¦
                1.0 / factor                                    # ä¸éœ€è¦ç¼©æ”¾çš„ç»´åº¦
            )
            freqs = freqs * scale

    # ç”Ÿæˆä½ç½®åºåˆ— [0, 1, 2, ..., end-1]
    t = torch.arange(end, device=freqs.device)
    
    # è®¡ç®—ä½ç½®å’Œé¢‘ç‡çš„å¤–ç§¯: (end,) x (dim/2,) -> (end, dim/2)
    freqs = torch.outer(t, freqs).float()
    
    # è®¡ç®— cos å’Œ sinï¼Œå¹¶å¤åˆ¶ä»¥åŒ¹é…å®Œæ•´ç»´åº¦
    # å½¢çŠ¶: (end, dim/2) -> (end, dim)
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """
    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ° Query å’Œ Key
    
    RoPE é€šè¿‡å°†å‘é‡åˆ†æˆä¸¤åŠï¼Œç„¶ååº”ç”¨æ—‹è½¬å˜æ¢æ¥ç¼–ç ä½ç½®ä¿¡æ¯ã€‚
    æ—‹è½¬å˜æ¢ä¿æŒå‘é‡çš„æ¨¡é•¿ä¸å˜ï¼Œåªæ”¹å˜æ–¹å‘ã€‚
    
    æ—‹è½¬å…¬å¼:
    å¯¹äºå‘é‡ x = [x_0, x_1, ..., x_{d-1}]ï¼Œåˆ†æˆä¸¤åŠ:
    - å‰åŠéƒ¨åˆ†: [x_0, x_1, ..., x_{d/2-1}]
    - ååŠéƒ¨åˆ†: [x_{d/2}, x_{d/2+1}, ..., x_{d-1}]
    
    æ—‹è½¬å:
    - æ–°å‰åŠéƒ¨åˆ† = å‰åŠéƒ¨åˆ† * cos - ååŠéƒ¨åˆ† * sin
    - æ–°ååŠéƒ¨åˆ† = ååŠéƒ¨åˆ† * cos + å‰åŠéƒ¨åˆ† * sin
    
    Args:
        q: Query å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch, seq_len, num_heads, head_dim)
        k: Key å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch, seq_len, num_kv_heads, head_dim)
        cos: ä½™å¼¦é¢‘ç‡ï¼Œå½¢çŠ¶ä¸º (seq_len, head_dim)
        sin: æ­£å¼¦é¢‘ç‡ï¼Œå½¢çŠ¶ä¸º (seq_len, head_dim)
        position_ids: ä½ç½® IDï¼ˆå¯é€‰ï¼Œå½“å‰æœªä½¿ç”¨ï¼‰
        unsqueeze_dim: åœ¨å“ªä¸ªç»´åº¦æ·»åŠ ç»´åº¦ä»¥è¿›è¡Œå¹¿æ’­
        
    Returns:
        Tuple[Tensor, Tensor]: åº”ç”¨ RoPE åçš„ (q, k)
    """
    def rotate_half(x):
        """
        å°†å‘é‡çš„å‰åä¸¤åŠäº¤æ¢å¹¶å–åå‰åŠéƒ¨åˆ†
        
        è¿™æ˜¯ RoPE æ—‹è½¬çš„å…³é”®æ“ä½œ:
        [x_0, x_1, ..., x_{d/2-1}, x_{d/2}, ..., x_{d-1}]
        -> [-x_{d/2}, ..., -x_{d-1}, x_0, ..., x_{d/2-1}]
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œæœ€åä¸€ç»´æ˜¯ head_dim
            
        Returns:
            æ—‹è½¬åçš„å¼ é‡
        """
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    # åº”ç”¨æ—‹è½¬: x_rotated = x * cos + rotate_half(x) * sin
    # è¿™ç­‰ä»·äºå¤æ•°ä¹˜æ³•: (a + bi) * (cos + i*sin) = (a*cos - b*sin) + i*(a*sin + b*cos)
    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    é‡å¤ Key/Value å¤´ä»¥åŒ¹é… Query å¤´çš„æ•°é‡ï¼ˆç”¨äº GQAï¼‰
    
    åœ¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ä¸­ï¼ŒKV å¤´çš„æ•°é‡å°‘äº Query å¤´ã€‚
    ä¸ºäº†è®¡ç®—æ³¨æ„åŠ›ï¼Œéœ€è¦å°† KV å¤´é‡å¤ä»¥åŒ¹é… Query å¤´çš„æ•°é‡ã€‚
    
    ä¾‹å¦‚: å¦‚æœæœ‰ 8 ä¸ª Query å¤´å’Œ 2 ä¸ª KV å¤´ï¼Œåˆ™æ¯ä¸ª KV å¤´éœ€è¦é‡å¤ 4 æ¬¡ã€‚
    
    è¿™ä¸ªå‡½æ•°ç­‰ä»·äº torch.repeat_interleave(x, dim=2, repeats=n_rep)ï¼Œ
    ä½†ä½¿ç”¨ expand + reshape å®ç°æ›´é«˜æ•ˆã€‚
    
    Args:
        x: KV å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch, seq_len, num_kv_heads, head_dim)
        n_rep: é‡å¤æ¬¡æ•°ï¼Œç­‰äº num_heads // num_kv_heads
        
    Returns:
        é‡å¤åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch, seq_len, num_heads, head_dim)
    """
    bs, slen, num_key_value_heads, head_dim = x.shape
    
    # å¦‚æœä¸éœ€è¦é‡å¤ï¼Œç›´æ¥è¿”å›
    if n_rep == 1:
        return x
    
    # ä½¿ç”¨ expand + reshape å®ç°é‡å¤
    # (bs, slen, num_kv_heads, head_dim) 
    # -> (bs, slen, num_kv_heads, 1, head_dim)
    # -> (bs, slen, num_kv_heads, n_rep, head_dim)
    # -> (bs, slen, num_kv_heads * n_rep, head_dim)
    return (
        x[:, :, :, None, :]
        .expand(bs, slen, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )


class Attention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    
    å®ç°äº†æ ‡å‡†çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼Œæ”¯æŒä»¥ä¸‹ç‰¹æ€§:
    1. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰: KV å¤´æ•°å¯ä»¥å°‘äº Query å¤´æ•°ï¼Œå‡å°‘ KV ç¼“å­˜
    2. Flash Attention: ä½¿ç”¨ PyTorch 2.0+ çš„ scaled_dot_product_attention åŠ é€Ÿ
    3. KV ç¼“å­˜: æ”¯æŒå¢é‡è§£ç ï¼Œç¼“å­˜å†å² KV ä»¥åŠ é€Ÿç”Ÿæˆ
    4. å› æœæ©ç : è‡ªåŠ¨åº”ç”¨å› æœæ©ç ï¼Œç¡®ä¿åªèƒ½çœ‹åˆ°ä¹‹å‰çš„ token
    
    æ³¨æ„åŠ›è®¡ç®—å…¬å¼:
    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V
    
    å…¶ä¸­:
    - Q: Queryï¼Œå½¢çŠ¶ä¸º (batch, num_heads, seq_len, head_dim)
    - K: Keyï¼Œå½¢çŠ¶ä¸º (batch, num_heads, seq_len, head_dim)
    - V: Valueï¼Œå½¢çŠ¶ä¸º (batch, num_heads, seq_len, head_dim)
    - d_k: head_dimï¼Œç”¨äºç¼©æ”¾é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
    
    Attributes:
        num_key_value_heads: KV å¤´æ•°é‡
        n_local_heads: Query å¤´æ•°é‡
        n_local_kv_heads: æœ¬åœ° KV å¤´æ•°é‡
        n_rep: KV å¤´é‡å¤æ¬¡æ•°
        head_dim: æ¯ä¸ªå¤´çš„ç»´åº¦
        q_proj, k_proj, v_proj: QKV æŠ•å½±å±‚
        o_proj: è¾“å‡ºæŠ•å½±å±‚
        attn_dropout, resid_dropout: Dropout å±‚
        flash: æ˜¯å¦ä½¿ç”¨ Flash Attention
    """
    
    def __init__(self, args: MiniMindConfig):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›å±‚
        
        Args:
            args: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        
        # è®¾ç½® KV å¤´æ•°é‡ï¼ˆGQA é…ç½®ï¼‰
        # å¦‚æœæœªæŒ‡å®š num_key_value_headsï¼Œåˆ™ä½¿ç”¨ä¸ num_attention_heads ç›¸åŒçš„å€¼ï¼ˆæ ‡å‡† MHAï¼‰
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        
        # ç¡®ä¿ Query å¤´æ•°æ˜¯ KV å¤´æ•°çš„æ•´æ•°å€
        assert args.num_attention_heads % self.num_key_value_heads == 0
        
        self.n_local_heads = args.num_attention_heads      # Query å¤´æ•°
        self.n_local_kv_heads = self.num_key_value_heads   # KV å¤´æ•°
        self.n_rep = self.n_local_heads // self.n_local_kv_heads  # KV é‡å¤æ¬¡æ•°
        self.head_dim = args.hidden_size // args.num_attention_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦
        
        # QKV æŠ•å½±å±‚ï¼ˆæ— åç½®ï¼Œéµå¾ª LLaMA è®¾è®¡ï¼‰
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        
        # Dropout å±‚
        self.attn_dropout = nn.Dropout(args.dropout)  # æ³¨æ„åŠ›æƒé‡ Dropout
        self.resid_dropout = nn.Dropout(args.dropout)  # æ®‹å·®è¿æ¥ Dropout
        self.dropout = args.dropout
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ Flash Attention
        # éœ€è¦ PyTorch >= 2.0 ä¸”é…ç½®ä¸­å¯ç”¨
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn

    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache=False,
                attention_mask: Optional[torch.Tensor] = None):
        """
        æ³¨æ„åŠ›å±‚å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            position_embeddings: RoPE ä½ç½®ç¼–ç  (cos, sin)
            past_key_value: ç¼“å­˜çš„ KVï¼Œç”¨äºå¢é‡è§£ç 
            use_cache: æ˜¯å¦è¿”å›æ›´æ–°åçš„ KV ç¼“å­˜
            attention_mask: æ³¨æ„åŠ›æ©ç ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼Œ0 è¡¨ç¤ºéœ€è¦æ©ç 
            
        Returns:
            Tuple[Tensor, Optional[Tuple[Tensor, Tensor]]]:
                - output: æ³¨æ„åŠ›è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
                - past_kv: æ›´æ–°åçš„ KV ç¼“å­˜ï¼ˆå¦‚æœ use_cache=Trueï¼‰
        """
        bsz, seq_len, _ = x.shape
        
        # è®¡ç®— Q, K, V æŠ•å½±
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼: (batch, seq_len, num_heads, head_dim)
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        # åº”ç”¨ RoPE ä½ç½®ç¼–ç 
        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # KV ç¼“å­˜å¤„ç†ï¼ˆç”¨äºå¢é‡è§£ç ï¼‰
        if past_key_value is not None:
            # å°†æ–°çš„ KV ä¸ç¼“å­˜çš„ KV æ‹¼æ¥
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        
        # å¦‚æœéœ€è¦ç¼“å­˜ï¼Œä¿å­˜å½“å‰ KV
        past_kv = (xk, xv) if use_cache else None

        # è½¬ç½®ä¸ºæ³¨æ„åŠ›è®¡ç®—æ ¼å¼: (batch, num_heads, seq_len, head_dim)
        # åŒæ—¶å¯¹ KV è¿›è¡Œé‡å¤ä»¥åŒ¹é… Query å¤´æ•°ï¼ˆGQAï¼‰
        xq, xk, xv = (
            xq.transpose(1, 2),
            repeat_kv(xk, self.n_rep).transpose(1, 2),
            repeat_kv(xv, self.n_rep).transpose(1, 2)
        )

        # é€‰æ‹©æ³¨æ„åŠ›è®¡ç®—æ–¹å¼
        if self.flash and seq_len > 1 and (attention_mask is None or torch.all(attention_mask == 1)):
            # ä½¿ç”¨ Flash Attentionï¼ˆæ›´å¿«ï¼Œæ›´çœå†…å­˜ï¼‰
            # æ¡ä»¶: å¯ç”¨ Flash Attentionï¼Œåºåˆ—é•¿åº¦ > 1ï¼Œä¸”æ²¡æœ‰è‡ªå®šä¹‰æ©ç 
            output = F.scaled_dot_product_attention(
                xq, xk, xv,
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=True  # è‡ªåŠ¨åº”ç”¨å› æœæ©ç 
            )
        else:
            # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T / sqrt(d_k)
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            
            # åº”ç”¨å› æœæ©ç ï¼ˆä¸Šä¸‰è§’çŸ©é˜µè®¾ä¸º -infï¼‰
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)

            # åº”ç”¨è‡ªå®šä¹‰æ³¨æ„åŠ›æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰
            if attention_mask is not None:
                # æ‰©å±•æ©ç ç»´åº¦ä»¥åŒ¹é…æ³¨æ„åŠ›åˆ†æ•°
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                # å°† 0 ä½ç½®è®¾ä¸ºå¤§è´Ÿæ•°ï¼Œä½¿ softmax åæ¥è¿‘ 0
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask

            # Softmax å½’ä¸€åŒ–ï¼ˆä½¿ç”¨ float32 ä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼‰
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            
            # åº”ç”¨æ³¨æ„åŠ› Dropout
            scores = self.attn_dropout(scores)
            
            # è®¡ç®—åŠ æƒå’Œ: Attention @ V
            output = scores @ xv

        # é‡å¡‘è¾“å‡º: (batch, num_heads, seq_len, head_dim) -> (batch, seq_len, hidden_size)
        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        
        # è¾“å‡ºæŠ•å½±å’Œæ®‹å·® Dropout
        output = self.resid_dropout(self.o_proj(output))
        
        return output, past_kv


class FeedForward(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFFNï¼‰
    
    ä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°çš„å‰é¦ˆç½‘ç»œï¼Œè¿™æ˜¯ LLaMA ç­‰ç°ä»£ LLM çš„æ ‡å‡†è®¾è®¡ã€‚
    
    SwiGLU å…¬å¼:
    FFN(x) = down_proj(act(gate_proj(x)) * up_proj(x))
    
    å…¶ä¸­:
    - gate_proj: é—¨æ§æŠ•å½±ï¼Œè¾“å‡ºç»´åº¦ä¸º intermediate_size
    - up_proj: ä¸ŠæŠ•å½±ï¼Œè¾“å‡ºç»´åº¦ä¸º intermediate_size
    - act: æ¿€æ´»å‡½æ•°ï¼ˆé»˜è®¤ SiLU/Swishï¼‰
    - down_proj: ä¸‹æŠ•å½±ï¼Œè¾“å‡ºç»´åº¦ä¸º hidden_size
    
    ä¸æ ‡å‡† FFN çš„åŒºåˆ«:
    - æ ‡å‡† FFN: down(act(up(x)))
    - SwiGLU: down(act(gate(x)) * up(x))
    
    SwiGLU é€šè¿‡é—¨æ§æœºåˆ¶æä¾›æ›´å¥½çš„æ¢¯åº¦æµåŠ¨å’Œè¡¨è¾¾èƒ½åŠ›ã€‚
    
    Attributes:
        gate_proj: é—¨æ§æŠ•å½±å±‚
        down_proj: ä¸‹æŠ•å½±å±‚
        up_proj: ä¸ŠæŠ•å½±å±‚
        dropout: Dropout å±‚
        act_fn: æ¿€æ´»å‡½æ•°
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–å‰é¦ˆç½‘ç»œ
        
        Args:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        
        # è®¡ç®—ä¸­é—´å±‚ç»´åº¦
        # å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨ hidden_size * 8/3ï¼Œå¹¶å¯¹é½åˆ° 64 çš„å€æ•°
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3)
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        
        # å®šä¹‰æŠ•å½±å±‚ï¼ˆæ— åç½®ï¼‰
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        
        # Dropout å’Œæ¿€æ´»å‡½æ•°
        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act]  # ä» HuggingFace è·å–æ¿€æ´»å‡½æ•°

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            
        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        # SwiGLU: down(act(gate(x)) * up(x))
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


class MoEGate(nn.Module):
    """
    æ··åˆä¸“å®¶é—¨æ§æœºåˆ¶ï¼ˆMoE Gateï¼‰
    
    é—¨æ§ç½‘ç»œè´Ÿè´£ä¸ºæ¯ä¸ª token é€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶ã€‚
    å®ƒè®¡ç®—æ¯ä¸ª token å¯¹æ‰€æœ‰ä¸“å®¶çš„äº²å’Œåº¦åˆ†æ•°ï¼Œç„¶åé€‰æ‹© top-k ä¸ªä¸“å®¶ã€‚
    
    å·¥ä½œæµç¨‹:
    1. å°†è¾“å…¥æŠ•å½±åˆ°ä¸“å®¶ç©ºé—´: scores = x @ weight^T
    2. åº”ç”¨è¯„åˆ†å‡½æ•°ï¼ˆå¦‚ softmaxï¼‰å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
    3. é€‰æ‹© top-k ä¸ªä¸“å®¶
    4. å¯é€‰åœ°å½’ä¸€åŒ–é€‰ä¸­ä¸“å®¶çš„æ¦‚ç‡
    5. è®¡ç®—è¾…åŠ©æŸå¤±ä»¥å¹³è¡¡ä¸“å®¶è´Ÿè½½
    
    è¾…åŠ©æŸå¤±:
    ä¸ºäº†é˜²æ­¢æ‰€æœ‰ token éƒ½è·¯ç”±åˆ°å°‘æ•°ä¸“å®¶ï¼ˆè´Ÿè½½ä¸å‡è¡¡ï¼‰ï¼Œ
    å¼•å…¥è¾…åŠ©æŸå¤±æ¥é¼“åŠ±å‡åŒ€çš„ä¸“å®¶ä½¿ç”¨ã€‚
    
    Attributes:
        top_k: æ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶æ•°é‡
        n_routed_experts: å¯è·¯ç”±ä¸“å®¶æ€»æ•°
        scoring_func: è¯„åˆ†å‡½æ•°ç±»å‹
        alpha: è¾…åŠ©æŸå¤±æƒé‡
        seq_aux: æ˜¯å¦ä½¿ç”¨åºåˆ—çº§è¾…åŠ©æŸå¤±
        norm_topk_prob: æ˜¯å¦å½’ä¸€åŒ– top-k æ¦‚ç‡
        weight: é—¨æ§æƒé‡çŸ©é˜µ
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–é—¨æ§ç½‘ç»œ
        
        Args:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok      # æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°
        self.n_routed_experts = config.n_routed_experts  # ä¸“å®¶æ€»æ•°

        self.scoring_func = config.scoring_func      # è¯„åˆ†å‡½æ•°
        self.alpha = config.aux_loss_alpha           # è¾…åŠ©æŸå¤±æƒé‡
        self.seq_aux = config.seq_aux                # åºåˆ—çº§è¾…åŠ©æŸå¤±

        self.norm_topk_prob = config.norm_topk_prob  # å½’ä¸€åŒ– top-k æ¦‚ç‡
        self.gating_dim = config.hidden_size         # é—¨æ§è¾“å…¥ç»´åº¦
        
        # é—¨æ§æƒé‡çŸ©é˜µ: (n_experts, hidden_size)
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        """
        åˆå§‹åŒ–é—¨æ§æƒé‡
        
        ä½¿ç”¨ Kaiming å‡åŒ€åˆå§‹åŒ–ï¼Œé€‚åˆ ReLU ç±»æ¿€æ´»å‡½æ•°
        """
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        """
        é—¨æ§å‰å‘ä¼ æ’­
        
        Args:
            hidden_states: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            
        Returns:
            Tuple[Tensor, Tensor, float]:
                - topk_idx: é€‰ä¸­ä¸“å®¶çš„ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (batch_size * seq_len, top_k)
                - topk_weight: é€‰ä¸­ä¸“å®¶çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (batch_size * seq_len, top_k)
                - aux_loss: è¾…åŠ©æŸå¤±å€¼
        """
        bsz, seq_len, h = hidden_states.shape
        
        # å±•å¹³ä¸º (batch_size * seq_len, hidden_size)
        hidden_states = hidden_states.view(-1, h)
        
        # è®¡ç®—é—¨æ§åˆ†æ•°: (batch_size * seq_len, n_experts)
        logits = F.linear(hidden_states, self.weight, None)
        
        # åº”ç”¨è¯„åˆ†å‡½æ•°
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1)
        else:
            raise NotImplementedError(f'ä¸æ”¯æŒçš„è¯„åˆ†å‡½æ•°: {self.scoring_func}')

        # é€‰æ‹© top-k ä¸ªä¸“å®¶
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        # å½’ä¸€åŒ– top-k æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
        if self.top_k > 1 and self.norm_topk_prob:
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator

        # è®¡ç®—è¾…åŠ©æŸå¤±ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼‰
        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1)
            
            if self.seq_aux:
                # åºåˆ—çº§è¾…åŠ©æŸå¤±
                # è®¡ç®—æ¯ä¸ªä¸“å®¶åœ¨æ¯ä¸ªåºåˆ—ä¸­è¢«é€‰ä¸­çš„é¢‘ç‡
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                ce.scatter_add_(
                    1, topk_idx_for_aux_loss,
                    torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)
                ).div_(seq_len * aux_topk / self.n_routed_experts)
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                # Token çº§è¾…åŠ©æŸå¤±
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                ce = mask_ce.float().mean(0)
                Pi = scores_for_aux.mean(0)
                fi = ce * self.n_routed_experts
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            aux_loss = 0
            
        return topk_idx, topk_weight, aux_loss


class MOEFeedForward(nn.Module):
    """
    æ··åˆä¸“å®¶å‰é¦ˆç½‘ç»œï¼ˆMoE FFNï¼‰
    
    MoE é€šè¿‡ä½¿ç”¨å¤šä¸ªä¸“å®¶ç½‘ç»œæ¥å¢åŠ æ¨¡å‹å®¹é‡ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—é‡ä¸å˜ã€‚
    æ¯ä¸ª token åªæ¿€æ´» top-k ä¸ªä¸“å®¶ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ä¸“å®¶ã€‚
    
    æ¶æ„:
    1. é—¨æ§ç½‘ç»œé€‰æ‹© top-k ä¸ªä¸“å®¶
    2. å°† token è·¯ç”±åˆ°é€‰ä¸­çš„ä¸“å®¶
    3. åŠ æƒåˆå¹¶ä¸“å®¶è¾“å‡º
    4. å¯é€‰åœ°æ·»åŠ å…±äº«ä¸“å®¶çš„è¾“å‡º
    
    ä¼˜ç‚¹:
    - å¢åŠ æ¨¡å‹å®¹é‡è€Œä¸å¢åŠ è®¡ç®—é‡
    - ä¸åŒä¸“å®¶å¯ä»¥ä¸“æ³¨äºä¸åŒç±»å‹çš„è¾“å…¥
    - æ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹
    
    Attributes:
        experts: ä¸“å®¶ç½‘ç»œåˆ—è¡¨
        gate: é—¨æ§ç½‘ç»œ
        shared_experts: å…±äº«ä¸“å®¶åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        aux_loss: è¾…åŠ©æŸå¤±ï¼ˆç”¨äºè®­ç»ƒï¼‰
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ– MoE å‰é¦ˆç½‘ç»œ
        
        Args:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        self.config = config
        
        # åˆ›å»ºä¸“å®¶ç½‘ç»œåˆ—è¡¨
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)
        ])
        
        # é—¨æ§ç½‘ç»œ
        self.gate = MoEGate(config)
        
        # å…±äº«ä¸“å®¶ï¼ˆæ‰€æœ‰ token éƒ½ä¼šç»è¿‡ï¼‰
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])

    def forward(self, x):
        """
        MoE å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            
        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        identity = x  # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºå…±äº«ä¸“å®¶
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        
        # ä½¿ç”¨é—¨æ§æœºåˆ¶é€‰æ‹©ä¸“å®¶
        topk_idx, topk_weight, aux_loss = self.gate(x)
        
        # å±•å¹³è¾“å…¥
        x = x.view(-1, x.shape[-1])
        flat_topk_idx = topk_idx.view(-1)
        
        if self.training:
            # è®­ç»ƒæ¨¡å¼: ä¸ºæ¯ä¸ª token å¤åˆ¶ top_k ä»½ï¼Œåˆ†åˆ«é€å…¥å¯¹åº”ä¸“å®¶
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
            y = torch.empty_like(x, dtype=torch.float16)
            
            # éå†æ¯ä¸ªä¸“å®¶ï¼Œå¤„ç†è·¯ç”±åˆ°è¯¥ä¸“å®¶çš„ token
            for i, expert in enumerate(self.experts):
                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to(y.dtype)
            
            # åŠ æƒåˆå¹¶ä¸“å®¶è¾“å‡º
            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
            y = y.view(*orig_shape)
        else:
            # æ¨ç†æ¨¡å¼: ä½¿ç”¨æ›´é«˜æ•ˆçš„å®ç°
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)
        
        # æ·»åŠ å…±äº«ä¸“å®¶çš„è¾“å‡º
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        
        # ä¿å­˜è¾…åŠ©æŸå¤±ä¾›è®­ç»ƒä½¿ç”¨
        self.aux_loss = aux_loss
        
        return y

    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        """
        MoE æ¨ç†ä¼˜åŒ–å®ç°
        
        åœ¨æ¨ç†æ—¶ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„æ‰¹å¤„ç†æ–¹å¼:
        1. æŒ‰ä¸“å®¶ç´¢å¼•æ’åºæ‰€æœ‰ token
        2. æ‰¹é‡å¤„ç†æ¯ä¸ªä¸“å®¶çš„ token
        3. ä½¿ç”¨ scatter_add åˆå¹¶ç»“æœ
        
        è¿™æ¯”è®­ç»ƒæ—¶çš„å®ç°æ›´é«˜æ•ˆï¼Œå› ä¸ºé¿å…äº†é‡å¤å¤åˆ¶ã€‚
        
        Args:
            x: å±•å¹³çš„è¾“å…¥ï¼Œå½¢çŠ¶ä¸º (batch_size * seq_len, hidden_size)
            flat_expert_indices: ä¸“å®¶ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (batch_size * seq_len * top_k,)
            flat_expert_weights: ä¸“å®¶æƒé‡ï¼Œå½¢çŠ¶ä¸º (batch_size * seq_len * top_k, 1)
            
        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size * seq_len, hidden_size)
        """
        expert_cache = torch.zeros_like(x)
        
        # æŒ‰ä¸“å®¶ç´¢å¼•æ’åº
        idxs = flat_expert_indices.argsort()
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶å¤„ç†çš„ token æ•°é‡çš„ç´¯ç§¯å’Œ
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        
        # è®¡ç®—åŸå§‹ token ç´¢å¼•
        token_idxs = idxs // self.config.num_experts_per_tok
        
        # éå†æ¯ä¸ªä¸“å®¶
        # ä¾‹å¦‚: tokens_per_expert = [6, 15, 20, 26] è¡¨ç¤º:
        # - ä¸“å®¶ 0 å¤„ç† token_idxs[:6]
        # - ä¸“å®¶ 1 å¤„ç† token_idxs[6:15]
        # - ä¸“å®¶ 2 å¤„ç† token_idxs[15:20]
        # - ä¸“å®¶ 3 å¤„ç† token_idxs[20:26]
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            
            # è·³è¿‡æ²¡æœ‰ token çš„ä¸“å®¶
            if start_idx == end_idx:
                continue
            
            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx:end_idx]
            expert_tokens = x[exp_token_idx]
            
            # ä¸“å®¶å¤„ç†å¹¶åŠ æƒ
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])
            
            # ä½¿ç”¨ scatter_add ç´¯åŠ åˆ°ç»“æœ
            expert_cache.scatter_add_(
                0,
                exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]),
                expert_out
            )

        return expert_cache


class MiniMindBlock(nn.Module):
    """
    MiniMind Transformer è§£ç å™¨å±‚
    
    æ¯ä¸ª Block åŒ…å«:
    1. è‡ªæ³¨æ„åŠ›å±‚ï¼ˆå¸¦æ®‹å·®è¿æ¥å’Œ Pre-Normï¼‰
    2. å‰é¦ˆç½‘ç»œï¼ˆå¸¦æ®‹å·®è¿æ¥å’Œ Pre-Normï¼‰
    
    Pre-Norm ç»“æ„:
    output = x + Attention(Norm(x))
    output = output + FFN(Norm(output))
    
    ä¸ Post-Norm çš„åŒºåˆ«:
    - Pre-Norm: Norm åœ¨å­å±‚ä¹‹å‰ï¼Œè®­ç»ƒæ›´ç¨³å®š
    - Post-Norm: Norm åœ¨å­å±‚ä¹‹åï¼Œå¯èƒ½éœ€è¦æ›´å°çš„å­¦ä¹ ç‡
    
    Attributes:
        self_attn: è‡ªæ³¨æ„åŠ›å±‚
        mlp: å‰é¦ˆç½‘ç»œï¼ˆæ™®é€š FFN æˆ– MoE FFNï¼‰
        input_layernorm: æ³¨æ„åŠ›å‰çš„ LayerNorm
        post_attention_layernorm: FFN å‰çš„ LayerNorm
    """
    
    def __init__(self, layer_id: int, config: MiniMindConfig):
        """
        åˆå§‹åŒ– Transformer å±‚
        
        Args:
            layer_id: å±‚ç´¢å¼•
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        
        # è‡ªæ³¨æ„åŠ›å±‚
        self.self_attn = Attention(config)

        self.layer_id = layer_id
        
        # Pre-Norm: åœ¨æ³¨æ„åŠ›å’Œ FFN ä¹‹å‰åº”ç”¨ LayerNorm
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # å‰é¦ˆç½‘ç»œ: æ ¹æ®é…ç½®é€‰æ‹©æ™®é€š FFN æˆ– MoE FFN
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        """
        Transformer å±‚å‰å‘ä¼ æ’­
        
        Args:
            hidden_states: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
            position_embeddings: RoPE ä½ç½®ç¼–ç  (cos, sin)
            past_key_value: ç¼“å­˜çš„ KVï¼ˆç”¨äºå¢é‡è§£ç ï¼‰
            use_cache: æ˜¯å¦è¿”å›æ›´æ–°åçš„ KV ç¼“å­˜
            attention_mask: æ³¨æ„åŠ›æ©ç 
            
        Returns:
            Tuple[Tensor, Optional[Tuple[Tensor, Tensor]]]:
                - hidden_states: è¾“å‡ºå¼ é‡
                - present_key_value: æ›´æ–°åçš„ KV ç¼“å­˜
        """
        # ä¿å­˜æ®‹å·®
        residual = hidden_states
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆPre-Normï¼‰
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states),
            position_embeddings,
            past_key_value,
            use_cache,
            attention_mask
        )
        
        # æ®‹å·®è¿æ¥
        hidden_states += residual
        
        # å‰é¦ˆç½‘ç»œï¼ˆPre-Norm + æ®‹å·®è¿æ¥ï¼‰
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        
        return hidden_states, present_key_value


class MiniMindModel(nn.Module):
    """
    MiniMind åŸºç¡€æ¨¡å‹
    
    å®Œæ•´çš„ Transformer è§£ç å™¨æ¨¡å‹ï¼ŒåŒ…å«:
    1. Token åµŒå…¥å±‚
    2. å¤šä¸ª Transformer è§£ç å™¨å±‚
    3. æœ€ç»ˆçš„ LayerNorm
    4. é¢„è®¡ç®—çš„ RoPE ä½ç½®ç¼–ç 
    
    è¿™æ˜¯ä¸€ä¸ªçº¯è§£ç å™¨æ¨¡å‹ï¼Œé€‚ç”¨äºå› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚
    
    Attributes:
        embed_tokens: Token åµŒå…¥å±‚
        dropout: åµŒå…¥ Dropout
        layers: Transformer å±‚åˆ—è¡¨
        norm: æœ€ç»ˆçš„ LayerNorm
        freqs_cos, freqs_sin: é¢„è®¡ç®—çš„ RoPE é¢‘ç‡
    """
    
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–åŸºç¡€æ¨¡å‹
        
        Args:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size
        self.num_hidden_layers = config.num_hidden_layers
        
        # Token åµŒå…¥å±‚
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        
        # åµŒå…¥ Dropout
        self.dropout = nn.Dropout(config.dropout)
        
        # Transformer è§£ç å™¨å±‚
        self.layers = nn.ModuleList([
            MiniMindBlock(l, config) for l in range(self.num_hidden_layers)
        ])
        
        # æœ€ç»ˆçš„ LayerNorm
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # é¢„è®¡ç®— RoPE ä½ç½®ç¼–ç 
        freqs_cos, freqs_sin = precompute_freqs_cis(
            dim=config.hidden_size // config.num_attention_heads,
            end=config.max_position_embeddings,
            rope_base=config.rope_theta,
            rope_scaling=config.rope_scaling
        )
        
        # æ³¨å†Œä¸º bufferï¼ˆä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼Œä½†ä¼šéšæ¨¡å‹ä¿å­˜/åŠ è½½ï¼‰
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        """
        åŸºç¡€æ¨¡å‹å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥ token IDï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len)
            attention_mask: æ³¨æ„åŠ›æ©ç ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆä½ç½®
            past_key_values: ç¼“å­˜çš„ KV åˆ—è¡¨ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰
            use_cache: æ˜¯å¦è¿”å›æ›´æ–°åçš„ KV ç¼“å­˜
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
            
        Returns:
            Tuple[Tensor, List, float]:
                - hidden_states: æœ€ç»ˆéšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, hidden_size)
                - presents: æ›´æ–°åçš„ KV ç¼“å­˜åˆ—è¡¨
                - aux_loss: MoE è¾…åŠ©æŸå¤±ï¼ˆå¦‚æœä½¿ç”¨ MoEï¼‰
        """
        batch_size, seq_length = input_ids.shape
        
        # å¤„ç† KV ç¼“å­˜
        # å¦‚æœ past_key_values æ˜¯ HuggingFace çš„ DynamicCacheï¼Œè½¬æ¢ä¸º None
        if hasattr(past_key_values, 'layers'):
            past_key_values = None
        past_key_values = past_key_values or [None] * len(self.layers)
        
        # è®¡ç®—èµ·å§‹ä½ç½®ï¼ˆç”¨äºå¢é‡è§£ç ï¼‰
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        # Token åµŒå…¥ + Dropout
        hidden_states = self.dropout(self.embed_tokens(input_ids))

        # è·å–å½“å‰ä½ç½®çš„ RoPE ç¼–ç 
        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        # éå†æ‰€æœ‰ Transformer å±‚
        presents = []
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)

        # æœ€ç»ˆçš„ LayerNorm
        hidden_states = self.norm(hidden_states)

        # è®¡ç®— MoE è¾…åŠ©æŸå¤±ï¼ˆå¦‚æœä½¿ç”¨ MoEï¼‰
        aux_loss = sum(
            layer.mlp.aux_loss
            for layer in self.layers
            if isinstance(layer.mlp, MOEFeedForward)
        )

        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    """
    MiniMind å› æœè¯­è¨€æ¨¡å‹
    
    åœ¨åŸºç¡€æ¨¡å‹ä¹‹ä¸Šæ·»åŠ è¯­è¨€æ¨¡å‹å¤´ï¼ˆLM Headï¼‰ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚
    ç»§æ‰¿è‡ª HuggingFace çš„ PreTrainedModel å’Œ GenerationMixinï¼Œ
    æ”¯æŒ HuggingFace ç”Ÿæ€ç³»ç»Ÿçš„æ‰€æœ‰åŠŸèƒ½ã€‚
    
    æ¶æ„:
    1. MiniMindModel: åŸºç¡€ Transformer æ¨¡å‹
    2. lm_head: çº¿æ€§å±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯è¡¨
    
    æƒé‡å…±äº«:
    lm_head çš„æƒé‡ä¸ embed_tokens å…±äº«ï¼Œå‡å°‘å‚æ•°é‡ã€‚
    
    Attributes:
        model: åŸºç¡€ Transformer æ¨¡å‹
        lm_head: è¯­è¨€æ¨¡å‹å¤´
        OUT: è¾“å‡ºå®¹å™¨
    """
    config_class = MiniMindConfig  # å…³è”é…ç½®ç±»

    def __init__(self, config: MiniMindConfig = None):
        """
        åˆå§‹åŒ–å› æœè¯­è¨€æ¨¡å‹
        
        Args:
            config: æ¨¡å‹é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or MiniMindConfig()
        super().__init__(self.config)
        
        # åŸºç¡€ Transformer æ¨¡å‹
        self.model = MiniMindModel(self.config)
        
        # è¯­è¨€æ¨¡å‹å¤´: hidden_size -> vocab_size
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        
        # æƒé‡å…±äº«: lm_head å’Œ embed_tokens å…±äº«æƒé‡
        # è¿™æ˜¯ä¸€ç§å¸¸è§çš„æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘å‚æ•°é‡å¹¶æé«˜æ€§èƒ½
        self.model.embed_tokens.weight = self.lm_head.weight
        
        # è¾“å‡ºå®¹å™¨ï¼ˆç”¨äºå­˜å‚¨å„ç§è¾“å‡ºï¼‰
        self.OUT = CausalLMOutputWithPast()

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        """
        å› æœè¯­è¨€æ¨¡å‹å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥ token IDï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len)
            attention_mask: æ³¨æ„åŠ›æ©ç 
            past_key_values: ç¼“å­˜çš„ KV
            use_cache: æ˜¯å¦ä½¿ç”¨ KV ç¼“å­˜
            logits_to_keep: ä¿ç•™å¤šå°‘ä¸ªä½ç½®çš„ logitsï¼ˆç”¨äºèŠ‚çœå†…å­˜ï¼‰
                - 0: ä¿ç•™æ‰€æœ‰
                - æ­£æ•´æ•° n: åªä¿ç•™æœ€å n ä¸ªä½ç½®
                - Tensor: è‡ªå®šä¹‰ç´¢å¼•
            **args: å…¶ä»–å‚æ•°
            
        Returns:
            CausalLMOutputWithPast: åŒ…å«ä»¥ä¸‹å­—æ®µ:
                - logits: é¢„æµ‹çš„ logitsï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, vocab_size)
                - past_key_values: æ›´æ–°åçš„ KV ç¼“å­˜
                - last_hidden_state: æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
                - aux_loss: MoE è¾…åŠ©æŸå¤±
        """
        # åŸºç¡€æ¨¡å‹å‰å‘ä¼ æ’­
        h, past_kvs, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        
        # ç¡®å®šè¦ä¿ç•™çš„ä½ç½®
        # è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–ï¼šåœ¨ç”Ÿæˆæ—¶ï¼Œé€šå¸¸åªéœ€è¦æœ€åä¸€ä¸ªä½ç½®çš„ logits
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        
        # è®¡ç®— logits: hidden_states -> vocab_size
        logits = self.lm_head(h[:, slice_indices, :])
        
        # å¡«å……è¾“å‡ºå®¹å™¨
        self.OUT.__setitem__('last_hidden_state', h)
        self.OUT.__setitem__('logits', logits)
        self.OUT.__setitem__('aux_loss', aux_loss)
        self.OUT.__setitem__('past_key_values', past_kvs)
        
        return self.OUT
