#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUå…¼å®¹æ€§æ£€æµ‹è„šæœ¬
ç”¨äºæ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦æ”¯æŒsm_120ç­‰æ–°æ¶æ„
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
from model.gpu_utils import print_gpu_info, get_gpu_compute_capability, check_flash_attention_support, get_recommended_dtype


def check_pytorch_version():
    """æ£€æŸ¥PyTorchç‰ˆæœ¬"""
    print("\n" + "="*60)
    print("ğŸ” PyTorch ç¯å¢ƒæ£€æµ‹")
    print("="*60)
    
    print(f"ğŸ“Œ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ“Œ CUDAå¯ç”¨: {'âœ… æ˜¯' if torch.cuda.is_available() else 'âŒ å¦'}")
    
    if torch.cuda.is_available():
        print(f"ğŸ“Œ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"ğŸ“Œ cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"ğŸ“Œ GPUæ•°é‡: {torch.cuda.device_count()}")
    else:
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        print("   å¦‚æœæ‚¨æœ‰NVIDIA GPUï¼Œè¯·æ£€æŸ¥CUDAå®‰è£…")
        return False
    
    return True


def check_dependencies():
    """æ£€æŸ¥å…¶ä»–ä¾èµ–"""
    print("\n" + "="*60)
    print("ğŸ” ä¾èµ–åŒ…æ£€æµ‹")
    print("="*60)
    
    deps = {
        'transformers': 'transformers',
        'numpy': 'numpy',
        'torch': 'torch'
    }
    
    for name, package in deps.items():
        try:
            mod = __import__(package)
            version = getattr(mod, '__version__', 'unknown')
            print(f"âœ… {name}: {version}")
        except ImportError:
            print(f"âŒ {name}: æœªå®‰è£…")


def run_simple_test():
    """è¿è¡Œç®€å•çš„CUDAæµ‹è¯•"""
    if not torch.cuda.is_available():
        return
    
    print("\n" + "="*60)
    print("ğŸ§ª CUDA åŠŸèƒ½æµ‹è¯•")
    print("="*60)
    
    try:
        # æµ‹è¯•åŸºæœ¬å¼ é‡æ“ä½œ
        x = torch.randn(100, 100).cuda()
        y = torch.randn(100, 100).cuda()
        z = torch.matmul(x, y)
        print("âœ… åŸºæœ¬CUDAå¼ é‡è¿ç®—: æ­£å¸¸")
        
        # æµ‹è¯•æ•°æ®ç±»å‹
        if torch.cuda.is_bf16_supported():
            x_bf16 = x.to(torch.bfloat16)
            y_bf16 = y.to(torch.bfloat16)
            z_bf16 = torch.matmul(x_bf16, y_bf16)
            print("âœ… BFloat16è¿ç®—: æ­£å¸¸")
        else:
            print("âš ï¸  BFloat16: ä¸æ”¯æŒ")
        
        # æµ‹è¯•Flash Attention
        if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
            q = torch.randn(2, 8, 32, 64).cuda()
            k = torch.randn(2, 8, 32, 64).cuda()
            v = torch.randn(2, 8, 32, 64).cuda()
            out = torch.nn.functional.scaled_dot_product_attention(q, k, v)
            print("âœ… Flash Attention (SDPA): å¯ç”¨")
        else:
            print("âš ï¸  Flash Attention: ä¸å¯ç”¨")
            
    except Exception as e:
        print(f"âŒ CUDAæµ‹è¯•å¤±è´¥: {e}")


def check_compute_capability_support():
    """æ£€æŸ¥è®¡ç®—èƒ½åŠ›æ”¯æŒæƒ…å†µ"""
    if not torch.cuda.is_available():
        return
    
    major, minor = get_gpu_compute_capability()
    
    print("\n" + "="*60)
    print("ğŸ¯ æ¶æ„æ”¯æŒæƒ…å†µ")
    print("="*60)
    
    # æ¶æ„æ˜ å°„
    arch_info = {
        (12, 0): ("Blackwell", "âœ… æœ€æ–°æ¶æ„", "RTX 5090, 5080"),
        (9, 0): ("Hopper", "âœ… æ•°æ®ä¸­å¿ƒçº§", "H100, H200"),
        (8, 9): ("Ada Lovelace", "âœ… æ¶ˆè´¹çº§æ——èˆ°", "RTX 4090, 4080, 4070"),
        (8, 6): ("Ampere", "âœ… æ¶ˆè´¹çº§", "RTX 3090, 3080, 3070"),
        (8, 0): ("Ampere", "âœ… æ•°æ®ä¸­å¿ƒçº§", "A100, A10"),
        (7, 5): ("Turing", "âœ… è¾ƒè€ä½†æ”¯æŒ", "RTX 2080, T4"),
        (7, 0): ("Volta", "âœ… è¾ƒè€ä½†æ”¯æŒ", "V100"),
    }
    
    arch_name, support, examples = arch_info.get(
        (major, minor), 
        ("æœªçŸ¥æ¶æ„", "âš ï¸  å¯èƒ½éœ€è¦æ›´æ–°PyTorch", "")
    )
    
    print(f"ğŸ“Œ æ¶æ„: {arch_name}")
    print(f"ğŸ“Œ æ”¯æŒçŠ¶æ€: {support}")
    if examples:
        print(f"ğŸ“Œ ä»£è¡¨GPU: {examples}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§
    if major >= 12:
        torch_version = torch.__version__.split('+')[0]
        major_v, minor_v = map(int, torch_version.split('.')[:2])
        if major_v < 2 or (major_v == 2 and minor_v < 7):
            print("\nâš ï¸  è­¦å‘Š: Blackwellæ¶æ„éœ€è¦PyTorch >= 2.7.0")
            print("   å½“å‰ç‰ˆæœ¬å¯èƒ½æ— æ³•å®Œå…¨åˆ©ç”¨GPUæ€§èƒ½")
            print("   å»ºè®®è¿è¡Œ: pip install --upgrade torch>=2.7.0 torchvision>=0.22.0")
        else:
            print("\nâœ… PyTorchç‰ˆæœ¬é€‚é…Blackwellæ¶æ„ï¼")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ MiniMind GPU å…¼å®¹æ€§æ£€æµ‹å·¥å…·")
    print("="*60)
    print("æœ¬å·¥å…·ç”¨äºæ£€æµ‹GPUæ¶æ„å’Œç¯å¢ƒé…ç½®")
    print("ç‰¹åˆ«æ”¯æŒæœ€æ–°çš„Blackwellæ¶æ„ (sm_120)")
    
    # 1. æ£€æŸ¥PyTorch
    if not check_pytorch_version():
        print("\nâŒ æ— æ³•ç»§ç»­æ£€æµ‹ï¼Œè¯·å…ˆå®‰è£…CUDAå’ŒPyTorch")
        print("   å®‰è£…æŒ‡å—: https://pytorch.org/get-started/locally/")
        return
    
    # 2. æ£€æŸ¥GPUè¯¦ç»†ä¿¡æ¯
    print_gpu_info()
    
    # 3. æ£€æŸ¥æ¶æ„æ”¯æŒ
    check_compute_capability_support()
    
    # 4. æ£€æŸ¥ä¾èµ–
    check_dependencies()
    
    # 5. è¿è¡ŒåŠŸèƒ½æµ‹è¯•
    run_simple_test()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("âœ… æ£€æµ‹å®Œæˆ")
    print("="*60)
    
    if torch.cuda.is_available():
        major, minor = get_gpu_compute_capability()
        flash_support = check_flash_attention_support()
        
        print("\nğŸ“‹ é…ç½®å»ºè®®:")
        print(f"   - dtype: {get_recommended_dtype()}")
        print(f"   - flash_attn: {flash_support}")
        
        if major >= 12:
            print("\nğŸ’¡ é’ˆå¯¹Blackwellæ¶æ„çš„å»ºè®®:")
            print("   1. ä½¿ç”¨bfloat16å¯è·å¾—æœ€ä½³æ€§èƒ½")
            print("   2. é€‚å½“å¢åŠ batch_sizeä»¥å……åˆ†åˆ©ç”¨æ˜¾å­˜å¸¦å®½")
            print("   3. ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œç¨‹åºä¼šè‡ªåŠ¨é€‚é…")
        
        print("\nğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼è¿è¡Œ:")
        print("   python trainer/train_pretrain.py --epochs 1 --batch_size 32")
    else:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    print("\n" + "="*60 + "\n")


if __name__ == "__main__":
    main()

