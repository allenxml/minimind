# MiniMind 推理与部署

本文档介绍 MiniMind 模型的推理和部署方案。

---

## 目录

1. [快速推理](#快速推理)
2. [推理参数详解](#推理参数详解)
3. [多轮对话](#多轮对话)
4. [LoRA 加载](#lora-加载)
5. [长序列处理](#长序列处理)
6. [性能优化](#性能优化)
7. [API 部署](#api-部署)
8. [常见问题](#常见问题)

---

## 快速推理

### 命令行推理

```bash
# 基础推理
python scripts/eval_llm.py

# 选择模式
# [0] 自动测试 - 使用预设问题
# [1] 手动输入 - 自由对话
```

### Python API 推理

```python
import torch
from transformers import AutoTokenizer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM

# 1. 加载模型
config = MiniMindConfig(hidden_size=512, num_hidden_layers=8)
model = MiniMindForCausalLM(config)
model.load_state_dict(torch.load('out/full_sft_512.pth'))
model = model.cuda().eval()

# 2. 加载 tokenizer
tokenizer = AutoTokenizer.from_pretrained('model')

# 3. 准备输入
messages = [{"role": "user", "content": "你好"}]
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(prompt, return_tensors="pt").to('cuda')

# 4. 生成回复
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.85,
        top_p=0.85
    )

# 5. 解码输出
response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
print(response)
```

---

## 推理参数详解

### 模型加载参数

| 参数 | 说明 | 示例 |
|------|------|------|
| `--load_from` | 加载路径 | `model`（原生）或 HuggingFace 路径 |
| `--save_dir` | 权重目录 | `out` |
| `--weight` | 权重名称 | `full_sft`, `dpo`, `reason` |
| `--lora_weight` | LoRA 权重 | `lora_identity`, `None` |

### 模型架构参数

| 参数 | 说明 | 可选值 |
|------|------|--------|
| `--hidden_size` | 隐藏层维度 | 512, 640, 768 |
| `--num_hidden_layers` | 层数 | 8, 16 |
| `--use_moe` | MoE 架构 | 0, 1 |

### 生成参数

| 参数 | 说明 | 推荐值 |
|------|------|--------|
| `--max_new_tokens` | 最大生成长度 | 512-8192 |
| `--temperature` | 温度 | 0.7-0.9 |
| `--top_p` | Top-p 采样 | 0.8-0.95 |
| `--historys` | 历史轮数 | 0-10 |

### 生成策略对比

| 策略 | temperature | top_p | 效果 |
|------|-------------|-------|------|
| 确定性 | 0.1 | 0.5 | 输出稳定，适合事实问答 |
| 平衡 | 0.7 | 0.85 | 默认推荐 |
| 创意 | 1.0 | 0.95 | 输出多样，适合创作 |

---

## 多轮对话

### 对话格式

```python
messages = [
    {"role": "system", "content": "你是一个有帮助的助手"},
    {"role": "user", "content": "你好"},
    {"role": "assistant", "content": "你好！有什么可以帮助你的？"},
    {"role": "user", "content": "介绍一下自己"}
]
```

### 历史管理

```bash
# 携带最近 4 轮对话
python scripts/eval_llm.py --historys 4
```

### 代码实现

```python
conversation = []

while True:
    user_input = input("用户: ")
    conversation.append({"role": "user", "content": user_input})
    
    # 只保留最近 N 轮
    if len(conversation) > max_history * 2:
        conversation = conversation[-max_history * 2:]
    
    # 生成回复
    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
    response = generate(prompt)
    
    conversation.append({"role": "assistant", "content": response})
    print(f"助手: {response}")
```

---

## LoRA 加载

### 命令行加载

```bash
# 加载身份认同 LoRA
python scripts/eval_llm.py --lora_weight lora_identity

# 加载医疗 LoRA
python scripts/eval_llm.py --lora_weight lora_medical
```

### Python API 加载

```python
from model.model_lora import apply_lora, load_lora

# 1. 加载基础模型
model = MiniMindForCausalLM(config)
model.load_state_dict(torch.load('out/full_sft_512.pth'))

# 2. 应用 LoRA 结构
apply_lora(model)

# 3. 加载 LoRA 权重
load_lora(model, 'out/lora/lora_identity_512.pth')

# 4. 推理
model = model.cuda().eval()
```

### LoRA 合并

```python
from model.model_lora import merge_lora

# 将 LoRA 权重合并到基础模型
merge_lora(model)

# 保存合并后的模型
torch.save(model.state_dict(), 'out/merged_model.pth')
```

---

## 长序列处理

### RoPE 外推

```bash
# 启用 4 倍位置外推
python scripts/eval_llm.py --inference_rope_scaling
```

### YaRN 配置

```python
config = MiniMindConfig(
    hidden_size=512,
    inference_rope_scaling=True  # 启用 YaRN
)

# 自动配置
# rope_scaling = {
#     "type": "yarn",
#     "factor": 4,
#     "original_max_position_embeddings": 2048
# }
```

### 注意事项

1. **外推 ≠ 长文本能力**：RoPE 外推只解决位置编码问题
2. **需要长文本训练**：真正的长文本能力需要用长序列训练
3. **显存增加**：长序列会显著增加显存使用

---

## 性能优化

### Flash Attention

```python
# 默认启用
config = MiniMindConfig(flash_attn=True)

# 禁用（兼容性问题时）
config = MiniMindConfig(flash_attn=False)
```

### KV 缓存

```python
# 推理时自动使用 KV 缓存
outputs = model.generate(
    **inputs,
    use_cache=True  # 默认启用
)
```

### 批量推理

```python
# 批量处理多个请求
prompts = ["问题1", "问题2", "问题3"]
inputs = tokenizer(prompts, return_tensors="pt", padding=True).to('cuda')

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=512)

for i, output in enumerate(outputs):
    print(f"回答{i+1}: {tokenizer.decode(output, skip_special_tokens=True)}")
```

### 半精度推理

```python
# 使用 float16 推理
model = model.half().cuda()

# 或使用 bfloat16
model = model.to(torch.bfloat16).cuda()
```

---

## API 部署

### FastAPI 示例

```python
from fastapi import FastAPI
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM

app = FastAPI()

# 加载模型
config = MiniMindConfig(hidden_size=512, num_hidden_layers=8)
model = MiniMindForCausalLM(config)
model.load_state_dict(torch.load('out/full_sft_512.pth'))
model = model.cuda().eval()
tokenizer = AutoTokenizer.from_pretrained('model')

class ChatRequest(BaseModel):
    messages: list
    max_tokens: int = 512
    temperature: float = 0.85

class ChatResponse(BaseModel):
    response: str

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    prompt = tokenizer.apply_chat_template(
        request.messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    inputs = tokenizer(prompt, return_tensors="pt").to('cuda')
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=request.max_tokens,
            temperature=request.temperature,
            do_sample=True
        )
    
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:], 
        skip_special_tokens=True
    )
    
    return ChatResponse(response=response)

# 运行: uvicorn api:app --host 0.0.0.0 --port 8000
```

### 调用示例

```python
import requests

response = requests.post(
    "http://localhost:8000/chat",
    json={
        "messages": [{"role": "user", "content": "你好"}],
        "max_tokens": 512,
        "temperature": 0.85
    }
)
print(response.json()["response"])
```

### Gradio 界面

```python
import gradio as gr
import torch
from transformers import AutoTokenizer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM

# 加载模型
config = MiniMindConfig(hidden_size=512, num_hidden_layers=8)
model = MiniMindForCausalLM(config)
model.load_state_dict(torch.load('out/full_sft_512.pth'))
model = model.cuda().eval()
tokenizer = AutoTokenizer.from_pretrained('model')

def chat(message, history):
    messages = []
    for h in history:
        messages.append({"role": "user", "content": h[0]})
        messages.append({"role": "assistant", "content": h[1]})
    messages.append({"role": "user", "content": message})
    
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to('cuda')
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.85, do_sample=True)
    
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response

demo = gr.ChatInterface(chat, title="MiniMind Chat")
demo.launch(share=True)
```

---

## 常见问题

### Q: 推理速度慢？

A: 优化方法：
1. 启用 Flash Attention
2. 使用半精度（float16/bfloat16）
3. 使用 KV 缓存
4. 减少 max_new_tokens

### Q: 显存不足？

A: 解决方案：
1. 使用更小的模型
2. 减少 batch_size
3. 使用半精度
4. 使用 CPU 推理（慢但省显存）

### Q: 输出重复？

A: 调整参数：
```bash
python scripts/eval_llm.py --temperature 0.9 --top_p 0.95
```

### Q: 输出不完整？

A: 增加生成长度：
```bash
python scripts/eval_llm.py --max_new_tokens 2048
```

### Q: 如何使用 CPU？

A: 指定设备：
```bash
python scripts/eval_llm.py --device cpu
```

### Q: 如何加载 HuggingFace 格式？

A: 指定路径：
```bash
python scripts/eval_llm.py --load_from path/to/hf_model
```

---

## 性能基准

### 推理速度（RTX 3090）

| 模型 | 精度 | 速度 (tokens/s) |
|------|------|-----------------|
| Small | fp16 | ~100 |
| Small | bf16 | ~95 |
| Base | fp16 | ~50 |
| MoE | fp16 | ~40 |

### 显存使用

| 模型 | 推理 (fp16) | 推理 (fp32) |
|------|-------------|-------------|
| Small | ~1GB | ~2GB |
| Base | ~2GB | ~4GB |
| MoE | ~3GB | ~6GB |