# MiniMind 模型架构

本文档详细介绍 MiniMind 的模型架构和技术细节。

---

## 目录

1. [架构概述](#架构概述)
2. [模型配置](#模型配置)
3. [核心组件](#核心组件)
4. [位置编码](#位置编码)
5. [注意力机制](#注意力机制)
6. [前馈网络](#前馈网络)
7. [MoE 架构](#moe-架构)
8. [模型规格](#模型规格)

---

## 架构概述

MiniMind 是一个基于 Transformer 的因果语言模型（Causal LM），采用 Decoder-only 架构。

### 整体结构

```
输入 Token IDs
      ↓
┌─────────────────┐
│  Token Embedding │
└─────────────────┘
      ↓
┌─────────────────┐
│  Dropout        │
└─────────────────┘
      ↓
┌─────────────────┐  ×N
│  MiniMindBlock  │ ←── Transformer 解码器层
└─────────────────┘
      ↓
┌─────────────────┐
│  RMSNorm        │
└─────────────────┘
      ↓
┌─────────────────┐
│  LM Head        │ ←── 与 Embedding 共享权重
└─────────────────┘
      ↓
输出 Logits
```

### 设计特点

1. **Pre-Norm 结构**：LayerNorm 在子层之前
2. **RMSNorm**：使用 RMSNorm 替代 LayerNorm
3. **RoPE**：旋转位置编码
4. **GQA**：分组查询注意力
5. **SwiGLU**：门控激活函数
6. **Flash Attention**：高效注意力实现

---

## 模型配置

### MiniMindConfig 参数

```python
class MiniMindConfig:
    # 基础架构参数
    hidden_size: int = 512          # 隐藏层维度
    num_hidden_layers: int = 8      # Transformer 层数
    num_attention_heads: int = 8    # 注意力头数
    num_key_value_heads: int = 2    # KV 头数（GQA）
    intermediate_size: int = None   # FFN 中间层维度（自动计算）
    vocab_size: int = 6400          # 词表大小
    
    # 位置编码
    max_position_embeddings: int = 32768  # 最大位置
    rope_theta: float = 1000000.0         # RoPE 基础频率
    
    # 正则化
    dropout: float = 0.0            # Dropout 概率
    rms_norm_eps: float = 1e-5      # RMSNorm epsilon
    
    # 特殊功能
    flash_attn: bool = True         # Flash Attention
    inference_rope_scaling: bool = False  # RoPE 外推
    
    # MoE 配置
    use_moe: bool = False           # 是否使用 MoE
    num_experts_per_tok: int = 2    # 每个 token 激活的专家数
    n_routed_experts: int = 4       # 可路由专家总数
    n_shared_experts: int = 1       # 共享专家数
```

### 配置示例

```python
# Small 模型 (26M)
config = MiniMindConfig(
    hidden_size=512,
    num_hidden_layers=8,
    num_attention_heads=8,
    num_key_value_heads=2
)

# Base 模型 (104M)
config = MiniMindConfig(
    hidden_size=768,
    num_hidden_layers=16,
    num_attention_heads=12,
    num_key_value_heads=4
)

# MoE 模型 (145M)
config = MiniMindConfig(
    hidden_size=640,
    num_hidden_layers=8,
    use_moe=True,
    n_routed_experts=4,
    num_experts_per_tok=2
)
```

---

## 核心组件

### RMSNorm

RMSNorm 是 LayerNorm 的简化版本，只进行缩放不进行中心化。

```python
class RMSNorm(nn.Module):
    def forward(self, x):
        # 计算 RMS
        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + eps)
        # 归一化并缩放
        return x / rms * self.weight
```

**公式**：
$$\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_i^2 + \epsilon}} \cdot \gamma$$

**优点**：
- 计算更简单
- 参数更少（无 bias）
- 效果与 LayerNorm 相当

### MiniMindBlock

每个 Transformer 层包含：

```python
class MiniMindBlock(nn.Module):
    def forward(self, x, position_embeddings):
        # 自注意力（Pre-Norm）
        residual = x
        x = self.input_layernorm(x)
        x = self.self_attn(x, position_embeddings)
        x = residual + x
        
        # 前馈网络（Pre-Norm）
        residual = x
        x = self.post_attention_layernorm(x)
        x = self.mlp(x)
        x = residual + x
        
        return x
```

---

## 位置编码

### RoPE (Rotary Position Embedding)

RoPE 通过旋转向量来编码位置信息。

**核心思想**：
- 将向量分成两半
- 对每一半应用旋转变换
- 旋转角度与位置成正比

**公式**：
$$\text{RoPE}(x, m) = \begin{pmatrix} x_0 \\ x_1 \\ \vdots \\ x_{d-1} \end{pmatrix} \odot \begin{pmatrix} \cos(m\theta_0) \\ \cos(m\theta_0) \\ \vdots \\ \cos(m\theta_{d/2-1}) \end{pmatrix} + \begin{pmatrix} -x_1 \\ x_0 \\ \vdots \\ x_{d-2} \end{pmatrix} \odot \begin{pmatrix} \sin(m\theta_0) \\ \sin(m\theta_0) \\ \vdots \\ \sin(m\theta_{d/2-1}) \end{pmatrix}$$

其中 $\theta_i = \text{base}^{-2i/d}$

**代码实现**：

```python
def apply_rotary_pos_emb(q, k, cos, sin):
    def rotate_half(x):
        return torch.cat((-x[..., x.shape[-1]//2:], x[..., :x.shape[-1]//2]), dim=-1)
    
    q_embed = q * cos + rotate_half(q) * sin
    k_embed = k * cos + rotate_half(k) * sin
    return q_embed, k_embed
```

### YaRN 位置外推

当 `inference_rope_scaling=True` 时，使用 YaRN 方法扩展位置编码：

```python
rope_scaling = {
    "type": "yarn",
    "factor": 4,                              # 外推倍数
    "original_max_position_embeddings": 2048, # 原始最大位置
    "beta_fast": 4,
    "beta_slow": 1
}
```

---

## 注意力机制

### 标准多头注意力

```python
# Q, K, V 投影
Q = x @ W_q  # (batch, seq, num_heads * head_dim)
K = x @ W_k  # (batch, seq, num_kv_heads * head_dim)
V = x @ W_v  # (batch, seq, num_kv_heads * head_dim)

# 注意力计算
scores = Q @ K.T / sqrt(head_dim)
scores = scores + causal_mask  # 因果掩码
attn = softmax(scores)
output = attn @ V
```

### GQA (Grouped Query Attention)

GQA 使用更少的 KV 头来减少 KV 缓存：

```
标准 MHA: num_heads = num_kv_heads = 8
GQA:      num_heads = 8, num_kv_heads = 2
```

**优点**：
- 减少 KV 缓存 4 倍
- 加速推理
- 效果接近 MHA

**实现**：

```python
def repeat_kv(x, n_rep):
    """重复 KV 头以匹配 Q 头数量"""
    if n_rep == 1:
        return x
    return x.repeat_interleave(n_rep, dim=2)
```

### Flash Attention

当 `flash_attn=True` 时，使用 PyTorch 2.0+ 的高效实现：

```python
if self.flash:
    output = F.scaled_dot_product_attention(
        q, k, v,
        dropout_p=self.dropout if self.training else 0.0,
        is_causal=True
    )
```

**优点**：
- 内存从 O(n²) 降到 O(n)
- 速度提升 2-4 倍
- 支持更长序列

---

## 前馈网络

### SwiGLU 激活

MiniMind 使用 SwiGLU 激活函数：

```python
class FeedForward(nn.Module):
    def forward(self, x):
        # SwiGLU: down(act(gate(x)) * up(x))
        return self.down_proj(
            self.act_fn(self.gate_proj(x)) * self.up_proj(x)
        )
```

**公式**：
$$\text{SwiGLU}(x) = \text{Swish}(xW_g) \odot (xW_u) \cdot W_d$$

**与标准 FFN 对比**：
- 标准 FFN: `down(act(up(x)))`
- SwiGLU: `down(act(gate(x)) * up(x))`

### 中间层维度

```python
# 自动计算中间层维度
if intermediate_size is None:
    intermediate_size = int(hidden_size * 8 / 3)
    intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)  # 对齐到 64
```

---

## MoE 架构

### 概述

MoE (Mixture of Experts) 通过多个专家网络增加模型容量：

```
输入
  ↓
┌─────────────┐
│   Gate      │ ←── 选择 top-k 专家
└─────────────┘
  ↓
┌─────────────┐
│  Expert 1   │
│  Expert 2   │ ←── 只激活选中的专家
│  Expert 3   │
│  Expert 4   │
└─────────────┘
  ↓
加权合并
  ↓
输出
```

### 门控机制

```python
class MoEGate(nn.Module):
    def forward(self, x):
        # 计算门控分数
        scores = F.softmax(x @ self.weight.T, dim=-1)
        
        # 选择 top-k 专家
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k)
        
        # 归一化权重
        topk_weight = topk_weight / topk_weight.sum(dim=-1, keepdim=True)
        
        return topk_idx, topk_weight
```

### 辅助损失

为了平衡专家负载，引入辅助损失：

```python
# 计算每个专家被选中的频率
expert_freq = topk_idx.bincount() / total_tokens

# 辅助损失：鼓励均匀分布
aux_loss = (expert_freq * scores.mean(0)).sum() * alpha
```

### 共享专家

除了可路由专家，还有共享专家（所有 token 都会经过）：

```python
if n_shared_experts > 0:
    for expert in self.shared_experts:
        output = output + expert(x)
```

---

## 模型规格

### 参数量计算

```python
# Embedding
embed_params = vocab_size * hidden_size

# 每层参数
attention_params = 4 * hidden_size * hidden_size  # Q, K, V, O
ffn_params = 3 * hidden_size * intermediate_size  # gate, up, down
layer_params = attention_params + ffn_params

# 总参数
total_params = embed_params + num_layers * layer_params
```

### 模型对照表

| 模型 | hidden_size | layers | heads | kv_heads | 参数量 |
|------|-------------|--------|-------|----------|--------|
| Small | 512 | 8 | 8 | 2 | 26M |
| Base | 768 | 16 | 12 | 4 | 104M |
| MoE | 640 | 8 | 8 | 2 | 145M |

### 显存需求

| 模型 | 推理 (fp16) | 训练 (bf16) |
|------|-------------|-------------|
| Small | ~1GB | ~2GB |
| Base | ~2GB | ~4GB |
| MoE | ~3GB | ~6GB |

---

## 代码结构

```
model/
├── model_minimind.py    # 核心模型实现
│   ├── MiniMindConfig   # 配置类
│   ├── RMSNorm          # 归一化层
│   ├── Attention        # 注意力层
│   ├── FeedForward      # 前馈网络
│   ├── MoEGate          # MoE 门控
│   ├── MOEFeedForward   # MoE 前馈网络
│   ├── MiniMindBlock    # Transformer 层
│   ├── MiniMindModel    # 基础模型
│   └── MiniMindForCausalLM  # 因果语言模型
├── model_lora.py        # LoRA 实现
└── gpu_utils.py         # GPU 工具