"""
è®­ç»ƒå·¥å…·å‡½æ•°é›†åˆ

æœ¬æ¨¡å—æä¾› MiniMind è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„å„ç§å·¥å…·å‡½æ•°å’Œç±»:

1. åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
   - is_main_process: æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
   - init_distributed_mode: åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
   
2. æ—¥å¿—å’Œæ£€æŸ¥ç‚¹
   - Logger: æ—¥å¿—æ‰“å°ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ‰“å°ï¼‰
   - lm_checkpoint: ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹
   
3. å­¦ä¹ ç‡è°ƒåº¦
   - get_lr: ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
   
4. æ¨¡å‹åˆå§‹åŒ–
   - init_model: åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer
   - setup_seed: è®¾ç½®éšæœºç§å­
   
5. æ•°æ®åŠ è½½
   - SkipBatchSampler: æ”¯æŒè·³è¿‡æ‰¹æ¬¡çš„é‡‡æ ·å™¨ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
   
6. SafeTensors æ”¯æŒ
   - save_model_safetensors: ä¿å­˜ä¸º SafeTensors æ ¼å¼
   - load_model_safetensors: ä» SafeTensors åŠ è½½

ä½¿ç”¨ç¤ºä¾‹:
    from trainer.trainer_utils import init_model, lm_checkpoint, Logger
    
    # åˆå§‹åŒ–æ¨¡å‹
    model, tokenizer = init_model(config, 'pretrain')
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    lm_checkpoint(config, model=model, optimizer=optimizer, epoch=0, step=100)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    ckp_data = lm_checkpoint(config)
"""

import os
import random
import math
import numpy as np
import torch
import torch.distributed as dist
from torch.utils.data import Sampler
from transformers import AutoTokenizer
from model.model_minimind import MiniMindForCausalLM
from model.gpu_utils import ensure_gpu_compatibility

# SafeTensors æ”¯æŒï¼ˆå¯é€‰ä¾èµ–ï¼‰
try:
    from safetensors.torch import save_file, load_file
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False


def is_main_process():
    """
    æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
    
    åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰ä¸»è¿›ç¨‹ï¼ˆrank=0ï¼‰åº”è¯¥æ‰§è¡ŒæŸäº›æ“ä½œï¼Œ
    å¦‚æ‰“å°æ—¥å¿—ã€ä¿å­˜æ£€æŸ¥ç‚¹ç­‰ã€‚
    
    Returns:
        bool: å¦‚æœæ˜¯ä¸»è¿›ç¨‹æˆ–éåˆ†å¸ƒå¼æ¨¡å¼ï¼Œè¿”å› True
        
    Example:
        >>> if is_main_process():
        ...     print("è¿™æ¡æ¶ˆæ¯åªåœ¨ä¸»è¿›ç¨‹æ‰“å°")
    """
    return not dist.is_initialized() or dist.get_rank() == 0


def Logger(content):
    """
    æ—¥å¿—æ‰“å°å‡½æ•°
    
    åªåœ¨ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—ï¼Œé¿å…åˆ†å¸ƒå¼è®­ç»ƒæ—¶é‡å¤æ‰“å°ã€‚
    
    Args:
        content: è¦æ‰“å°çš„å†…å®¹
        
    Example:
        >>> Logger(f"Epoch 1, Loss: 0.5")
        Epoch 1, Loss: 0.5  # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤º
    """
    if is_main_process():
        print(content)


def get_lr(current_step, total_steps, lr):
    """
    ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
    
    å­¦ä¹ ç‡ä» lr å¼€å§‹ï¼ŒæŒ‰ä½™å¼¦æ›²çº¿è¡°å‡åˆ° lr/10ã€‚
    è¿™ç§è°ƒåº¦æ–¹å¼åœ¨è®­ç»ƒåæœŸæä¾›æ›´å°çš„å­¦ä¹ ç‡ï¼Œæœ‰åŠ©äºæ¨¡å‹æ”¶æ•›ã€‚
    
    å…¬å¼:
    lr_t = lr/10 + 0.5 * lr * (1 + cos(Ï€ * t / T))
    
    å…¶ä¸­:
    - t: å½“å‰æ­¥æ•°
    - T: æ€»æ­¥æ•°
    - lr: åˆå§‹å­¦ä¹ ç‡
    
    Args:
        current_step (int): å½“å‰è®­ç»ƒæ­¥æ•°
        total_steps (int): æ€»è®­ç»ƒæ­¥æ•°
        lr (float): åˆå§‹å­¦ä¹ ç‡
        
    Returns:
        float: å½“å‰æ­¥çš„å­¦ä¹ ç‡
        
    Example:
        >>> lr = get_lr(500, 1000, 1e-4)
        >>> print(f"å½“å‰å­¦ä¹ ç‡: {lr}")
    """
    # ä½™å¼¦é€€ç«: ä» lr è¡°å‡åˆ° lr/10
    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))


def init_distributed_mode():
    """
    åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    
    æ£€æµ‹ç¯å¢ƒå˜é‡åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†å¸ƒå¼è®­ç»ƒï¼Œå¦‚æœæ˜¯åˆ™åˆå§‹åŒ– NCCL åç«¯ã€‚
    
    åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå˜é‡:
    - RANK: å…¨å±€è¿›ç¨‹æ’å
    - LOCAL_RANK: æœ¬åœ° GPU æ’å
    - WORLD_SIZE: æ€»è¿›ç¨‹æ•°
    
    Returns:
        int: æœ¬åœ° GPU æ’åï¼ˆéåˆ†å¸ƒå¼æ¨¡å¼è¿”å› 0ï¼‰
        
    Example:
        >>> local_rank = init_distributed_mode()
        >>> device = f"cuda:{local_rank}"
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å¸ƒå¼æ¨¡å¼
    if int(os.environ.get("RANK", -1)) == -1:
        return 0  # é DDP æ¨¡å¼

    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(backend="nccl")
    
    # è·å–æœ¬åœ° GPU æ’åå¹¶è®¾ç½®è®¾å¤‡
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    
    return local_rank


def setup_seed(seed: int):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    
    è®¾ç½® Pythonã€NumPyã€PyTorch çš„éšæœºç§å­ï¼Œ
    å¹¶é…ç½® cuDNN ä¸ºç¡®å®šæ€§æ¨¡å¼ã€‚
    
    Args:
        seed (int): éšæœºç§å­å€¼
        
    Note:
        ç¡®å®šæ€§æ¨¡å¼å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œä½†èƒ½ä¿è¯ç»“æœå¯é‡å¤ã€‚
        
    Example:
        >>> setup_seed(42)
        >>> # ç°åœ¨æ‰€æœ‰éšæœºæ“ä½œéƒ½æ˜¯å¯é‡å¤çš„
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # å¤š GPU æƒ…å†µ
    
    # ç¡®å®šæ€§æ¨¡å¼
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def _clean_old_checkpoints(save_dir, weight, hidden_size, moe_path, keep_last_n):
    """
    æ¸…ç†æ—§çš„ checkpoint æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘ N ä¸ª
    
    åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿå¤§é‡æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œè¿™ä¸ªå‡½æ•°å¸®åŠ©ç®¡ç†ç£ç›˜ç©ºé—´ã€‚
    
    Args:
        save_dir (str): checkpoint ç›®å½•
        weight (str): æƒé‡åç§°å‰ç¼€
        hidden_size (int): éšè—å±‚å¤§å°
        moe_path (str): MoE åç¼€ï¼ˆ'_moe' æˆ– ''ï¼‰
        keep_last_n (int): ä¿ç•™æœ€è¿‘ N ä¸ª checkpoint
        
    Example:
        >>> _clean_old_checkpoints('checkpoints', 'pretrain', 512, '', 3)
        ğŸ—‘ï¸  å·²åˆ é™¤æ—§checkpoint: pretrain_512_step100.pth
    """
    import glob
    
    # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„ checkpoint æ–‡ä»¶
    pattern_pth = f'{save_dir}/{weight}_{hidden_size}{moe_path}_step*.pth'
    pattern_safetensors = f'{save_dir}/{weight}_{hidden_size}{moe_path}_step*.safetensors'
    pattern_resume = f'{save_dir}/{weight}_{hidden_size}{moe_path}_step*_resume.pth'
    
    # è·å–æ‰€æœ‰ checkpoint æ–‡ä»¶ï¼ˆä¸åŒ…æ‹¬ resume æ–‡ä»¶ï¼‰
    ckpt_files_pth = glob.glob(pattern_pth)
    ckpt_files_safetensors = glob.glob(pattern_safetensors)
    resume_files = glob.glob(pattern_resume)
    
    # è¿‡æ»¤æ‰ resume æ–‡ä»¶
    ckpt_files_pth = [f for f in ckpt_files_pth if '_resume.pth' not in f]
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    ckpt_files_pth.sort(key=os.path.getmtime, reverse=True)
    ckpt_files_safetensors.sort(key=os.path.getmtime, reverse=True)
    resume_files.sort(key=os.path.getmtime, reverse=True)
    
    # åˆ é™¤æ—§çš„ checkpoint
    deleted_count = 0
    for files_list in [ckpt_files_pth, ckpt_files_safetensors, resume_files]:
        if len(files_list) > keep_last_n:
            for old_file in files_list[keep_last_n:]:
                try:
                    os.remove(old_file)
                    deleted_count += 1
                    Logger(f"ğŸ—‘ï¸  å·²åˆ é™¤æ—§checkpoint: {os.path.basename(old_file)}")
                except Exception as e:
                    Logger(f"âš ï¸  åˆ é™¤å¤±è´¥ {old_file}: {e}")
    
    if deleted_count > 0:
        Logger(f"âœ… æ¸…ç†å®Œæˆï¼Œä¿ç•™æœ€è¿‘ {keep_last_n} ä¸ªcheckpoint")


def lm_checkpoint(lm_config, weight='full_sft', model=None, optimizer=None, epoch=0, step=0, wandb=None, 
                  save_dir='../checkpoints', save_safetensors=True, keep_last_n=3, save_with_step=True, **kwargs):
    """
    ä¿å­˜æˆ–åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
    
    è¿™æ˜¯ä¸€ä¸ªåŒå‘å‡½æ•°:
    - å½“ model ä¸ä¸º None æ—¶: ä¿å­˜æ£€æŸ¥ç‚¹
    - å½“ model ä¸º None æ—¶: åŠ è½½æ£€æŸ¥ç‚¹
    
    ä¿å­˜çš„å†…å®¹:
    1. æ¨¡å‹æƒé‡ï¼ˆ.pth å’Œå¯é€‰çš„ .safetensorsï¼‰
    2. æ¢å¤ç‚¹ï¼ˆåŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ã€epochã€step ç­‰ï¼‰
    
    Args:
        lm_config: æ¨¡å‹é…ç½®å¯¹è±¡
        weight (str): æƒé‡åç§°å‰ç¼€ï¼ˆå¦‚ 'pretrain', 'full_sft'ï¼‰
        model: æ¨¡å‹å¯¹è±¡ï¼ˆNone è¡¨ç¤ºåŠ è½½æ¨¡å¼ï¼‰
        optimizer: ä¼˜åŒ–å™¨å¯¹è±¡
        epoch (int): å½“å‰ epoch
        step (int): å½“å‰ step
        wandb: wandb/swanlab å¯¹è±¡ï¼ˆç”¨äºä¿å­˜ run IDï¼‰
        save_dir (str): ä¿å­˜ç›®å½•
        save_safetensors (bool): æ˜¯å¦åŒæ—¶ä¿å­˜ SafeTensors æ ¼å¼
        keep_last_n (int): ä¿ç•™æœ€è¿‘ N ä¸ª checkpointï¼ˆ0 è¡¨ç¤ºå…¨éƒ¨ä¿ç•™ï¼‰
        save_with_step (bool): æ˜¯å¦åœ¨æ–‡ä»¶åä¸­åŒ…å«æ­¥æ•°
        **kwargs: å…¶ä»–éœ€è¦ä¿å­˜çš„å¯¹è±¡ï¼ˆå¦‚ scaler, schedulerï¼‰
        
    Returns:
        dict or None: åŠ è½½æ¨¡å¼è¿”å›æ£€æŸ¥ç‚¹æ•°æ®ï¼Œä¿å­˜æ¨¡å¼è¿”å› None
        
    Example:
        # ä¿å­˜æ£€æŸ¥ç‚¹
        >>> lm_checkpoint(config, model=model, optimizer=optimizer, epoch=0, step=100)
        ğŸ’¾ å·²ä¿å­˜æ¨¡å‹: checkpoints/pretrain_512_step100.pth
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        >>> ckp_data = lm_checkpoint(config)
        >>> model.load_state_dict(ckp_data['model'])
    """
    os.makedirs(save_dir, exist_ok=True)
    moe_path = '_moe' if lm_config.use_moe else ''
    
    # æ ¹æ®é…ç½®å†³å®šæ–‡ä»¶å
    if save_with_step and step > 0:
        # å¸¦æ­¥æ•°çš„æ–‡ä»¶åï¼špretrain_512_step3600.pth
        ckp_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_step{step}.pth'
        resume_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_step{step}_resume.pth'
    else:
        # ä¼ ç»Ÿæ–‡ä»¶åï¼ˆä¼šè¦†ç›–ï¼‰
        ckp_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}.pth'
        resume_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_resume.pth'

    if model is not None:
        # ========== ä¿å­˜æ¨¡å¼ ==========
        from torch.nn.parallel import DistributedDataParallel
        
        # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
        state_dict = model.module.state_dict() if isinstance(model, DistributedDataParallel) else model.state_dict()
        
        # ä¿å­˜ä¸º .pth æ ¼å¼ï¼ˆåŠç²¾åº¦ä»¥èŠ‚çœç©ºé—´ï¼‰
        ckp_tmp = ckp_path + '.tmp'
        torch.save({k: v.half() for k, v in state_dict.items()}, ckp_tmp)
        os.replace(ckp_tmp, ckp_path)  # åŸå­æ“ä½œï¼Œé˜²æ­¢å†™å…¥ä¸­æ–­
        Logger(f"ğŸ’¾ å·²ä¿å­˜æ¨¡å‹: {ckp_path}")
        
        # åŒæ—¶ä¿å­˜ä¸º .safetensors æ ¼å¼ï¼ˆå¦‚æœå¯ç”¨ä¸”å¯ç”¨ï¼‰
        if save_safetensors and SAFETENSORS_AVAILABLE:
            if save_with_step and step > 0:
                safetensors_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_step{step}.safetensors'
            else:
                safetensors_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}.safetensors'
            save_model_safetensors(model, safetensors_path, half_precision=True)
        
        # è·å– wandb run IDï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        wandb_id = None
        if wandb:
            if hasattr(wandb, 'get_run'):
                run = wandb.get_run()
                wandb_id = getattr(run, 'id', None) if run else None
            else:
                wandb_id = getattr(wandb, 'id', None)

        # æ„å»ºæ¢å¤æ•°æ®
        resume_data = {
            'model': state_dict,
            'optimizer': optimizer.state_dict(),
            'epoch': epoch,
            'step': step,
            'world_size': dist.get_world_size() if dist.is_initialized() else 1,
            'wandb_id': wandb_id
        }
        
        # ä¿å­˜é¢å¤–çš„å¯¹è±¡ï¼ˆå¦‚ scaler, schedulerï¼‰
        for key, value in kwargs.items():
            if value is not None:
                if hasattr(value, 'state_dict'):
                    if isinstance(value, DistributedDataParallel):
                        resume_data[key] = value.module.state_dict()
                    else:
                        resume_data[key] = value.state_dict()
                else:
                    resume_data[key] = value

        # ä¿å­˜æ¢å¤ç‚¹
        resume_tmp = resume_path + '.tmp'
        torch.save(resume_data, resume_tmp)
        os.replace(resume_tmp, resume_path)
        Logger(f"ğŸ’¾ å·²ä¿å­˜æ¢å¤ç‚¹: {resume_path}")
        
        # æ¸…ç†æ—§çš„ checkpointï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if keep_last_n > 0 and save_with_step:
            _clean_old_checkpoints(save_dir, weight, lm_config.hidden_size, moe_path, keep_last_n)
            
    else:
        # ========== åŠ è½½æ¨¡å¼ ==========
        import glob
        
        # å…ˆå°è¯•ä¼ ç»Ÿæ–‡ä»¶å
        if os.path.exists(resume_path):
            latest_resume = resume_path
        else:
            # æŸ¥æ‰¾æ‰€æœ‰å¸¦æ­¥æ•°çš„ resume æ–‡ä»¶
            pattern = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_step*_resume.pth'
            resume_files = glob.glob(pattern)
            
            if resume_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                latest_resume = max(resume_files, key=os.path.getmtime)
                Logger(f"ğŸ“‚ æ‰¾åˆ°æœ€æ–°checkpoint: {os.path.basename(latest_resume)}")
            else:
                Logger(f"âš ï¸  æœªæ‰¾åˆ°checkpoint: {resume_path}")
                return None
        
        # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
        ckp_data = torch.load(latest_resume, map_location='cpu')
        
        # å¤„ç† GPU æ•°é‡å˜åŒ–çš„æƒ…å†µ
        saved_ws = ckp_data.get('world_size', 1)
        current_ws = dist.get_world_size() if dist.is_initialized() else 1
        if saved_ws != current_ws:
            # è°ƒæ•´ step ä»¥é€‚åº”æ–°çš„ GPU æ•°é‡
            ckp_data['step'] = ckp_data['step'] * saved_ws // current_ws
            Logger(f'GPUæ•°é‡å˜åŒ–({saved_ws}â†’{current_ws})ï¼Œstepå·²è‡ªåŠ¨è½¬æ¢ä¸º{ckp_data["step"]}')
        
        return ckp_data


def save_model_safetensors(model, save_path, half_precision=True):
    """
    ä¿å­˜æ¨¡å‹ä¸º SafeTensors æ ¼å¼
    
    SafeTensors æ˜¯ä¸€ç§å®‰å…¨ã€å¿«é€Ÿçš„æ¨¡å‹æƒé‡æ ¼å¼:
    - å®‰å…¨: çº¯æ•°æ®æ ¼å¼ï¼Œä¸ä¼šæ‰§è¡Œæ¶æ„ä»£ç 
    - å¿«é€Ÿ: åŠ è½½é€Ÿåº¦æ¯” pickle å¿« 2-3 å€
    - è·¨æ¡†æ¶: æ”¯æŒ PyTorch/TensorFlow/JAX
    
    Args:
        model: æ¨¡å‹å¯¹è±¡
        save_path (str): ä¿å­˜è·¯å¾„ï¼ˆ.safetensorsï¼‰
        half_precision (bool): æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦ä¿å­˜
        
    Returns:
        bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        
    Example:
        >>> save_model_safetensors(model, 'model.safetensors')
        âœ… æ¨¡å‹å·²ä¿å­˜ä¸º SafeTensors: model.safetensors
    """
    if not SAFETENSORS_AVAILABLE:
        Logger("âš ï¸  safetensors æœªå®‰è£…ï¼Œè·³è¿‡ .safetensors ä¿å­˜")
        Logger("   å®‰è£…æ–¹æ³•: pip install safetensors")
        return False
    
    try:
        from torch.nn.parallel import DistributedDataParallel
        
        # è·å– state_dict
        if isinstance(model, DistributedDataParallel):
            state_dict = model.module.state_dict()
        else:
            state_dict = model.state_dict()
        
        # è½¬æ¢ä¸ºåŠç²¾åº¦å¹¶ç¡®ä¿è¿ç»­å­˜å‚¨ï¼ˆsafetensors è¦æ±‚ï¼‰
        if half_precision:
            state_dict = {k: v.half().contiguous() for k, v in state_dict.items()}
        else:
            state_dict = {k: v.contiguous() for k, v in state_dict.items()}
        
        # ä¿å­˜
        save_file(state_dict, save_path)
        Logger(f"âœ… æ¨¡å‹å·²ä¿å­˜ä¸º SafeTensors: {save_path}")
        return True
        
    except Exception as e:
        Logger(f"âŒ SafeTensors ä¿å­˜å¤±è´¥: {e}")
        return False


def load_model_safetensors(save_path, device='cpu'):
    """
    ä» SafeTensors æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡
    
    Args:
        save_path (str): .safetensors æ–‡ä»¶è·¯å¾„
        device (str): åŠ è½½åˆ°çš„è®¾å¤‡
        
    Returns:
        dict: state_dict
        
    Raises:
        ImportError: å¦‚æœ safetensors æœªå®‰è£…
        
    Example:
        >>> state_dict = load_model_safetensors('model.safetensors')
        >>> model.load_state_dict(state_dict)
    """
    if not SAFETENSORS_AVAILABLE:
        raise ImportError("safetensors æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install safetensors")
    
    try:
        state_dict = load_file(save_path, device=str(device))
        Logger(f"âœ… ä» SafeTensors åŠ è½½æ¨¡å‹: {save_path}")
        return state_dict
    except Exception as e:
        Logger(f"âŒ SafeTensors åŠ è½½å¤±è´¥: {e}")
        raise


def init_model(lm_config, from_weight='pretrain', tokenizer_path='../model', save_dir='../out', 
               device='cuda', auto_gpu_config=True, prefer_safetensors=True):
    """
    åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer
    
    è¿™æ˜¯æ¨¡å‹åˆå§‹åŒ–çš„ä¸»è¦å…¥å£å‡½æ•°ï¼Œå¤„ç†:
    1. GPU å…¼å®¹æ€§æ£€æµ‹å’Œé…ç½®
    2. Tokenizer åŠ è½½
    3. æ¨¡å‹åˆ›å»º
    4. æƒé‡åŠ è½½ï¼ˆæ”¯æŒ .pth å’Œ .safetensorsï¼‰
    
    Args:
        lm_config: æ¨¡å‹é…ç½®å¯¹è±¡
        from_weight (str): è¦åŠ è½½çš„æƒé‡åç§°ï¼ˆ'none' è¡¨ç¤ºä¸åŠ è½½ï¼‰
        tokenizer_path (str): tokenizer è·¯å¾„
        save_dir (str): æƒé‡ä¿å­˜ç›®å½•
        device (str): ç›®æ ‡è®¾å¤‡
        auto_gpu_config (bool): æ˜¯å¦è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½® GPU å…¼å®¹æ€§
        prefer_safetensors (bool): æ˜¯å¦ä¼˜å…ˆåŠ è½½ .safetensors æ ¼å¼
        
    Returns:
        Tuple[model, tokenizer]: åˆå§‹åŒ–åçš„æ¨¡å‹å’Œ tokenizer
        
    Example:
        >>> config = MiniMindConfig(hidden_size=512)
        >>> model, tokenizer = init_model(config, 'pretrain')
        æ‰€åŠ è½½Modelå¯è®­ç»ƒå‚æ•°ï¼š26.000 ç™¾ä¸‡
    """
    # GPU å…¼å®¹æ€§æ£€æµ‹å’Œè‡ªåŠ¨é…ç½®ï¼ˆæ”¯æŒ sm_120 Blackwell æ¶æ„ï¼‰
    if auto_gpu_config and 'cuda' in device:
        lm_config = ensure_gpu_compatibility(lm_config)
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    
    # åˆ›å»ºæ¨¡å‹
    model = MiniMindForCausalLM(lm_config)

    if from_weight != 'none':
        moe_suffix = '_moe' if lm_config.use_moe else ''
        
        # å°è¯•åŠ è½½æƒé‡
        weights = None
        weight_path = None
        
        # ä¼˜å…ˆå°è¯•åŠ è½½ .safetensors æ ¼å¼
        if prefer_safetensors and SAFETENSORS_AVAILABLE:
            safetensors_path = f'{save_dir}/{from_weight}_{lm_config.hidden_size}{moe_suffix}.safetensors'
            if os.path.exists(safetensors_path):
                try:
                    weights = load_model_safetensors(safetensors_path, device=device)
                    weight_path = safetensors_path
                except Exception as e:
                    Logger(f"âš ï¸  SafeTensors åŠ è½½å¤±è´¥ï¼Œå°è¯• .pth æ ¼å¼: {e}")
        
        # å¦‚æœæ²¡æœ‰åŠ è½½æˆåŠŸï¼Œå›é€€åˆ° .pth æ ¼å¼
        if weights is None:
            pth_path = f'{save_dir}/{from_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
            if os.path.exists(pth_path):
                weights = torch.load(pth_path, map_location=device)
                weight_path = pth_path
                Logger(f"ä» PyTorch æ ¼å¼åŠ è½½: {pth_path}")
            else:
                Logger(f"âš ï¸  æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {pth_path}")
        
        # åŠ è½½æƒé‡
        if weights is not None:
            model.load_state_dict(weights, strict=False)
        else:
            Logger(f"âš ï¸  æœªæ‰¾åˆ°å¯ç”¨çš„æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    Logger(f'æ‰€åŠ è½½Modelå¯è®­ç»ƒå‚æ•°ï¼š{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} ç™¾ä¸‡')
    
    return model.to(device), tokenizer


class SkipBatchSampler(Sampler):
    """
    æ”¯æŒè·³è¿‡æ‰¹æ¬¡çš„é‡‡æ ·å™¨
    
    ç”¨äºæ–­ç‚¹ç»­è®­æ—¶è·³è¿‡å·²ç»è®­ç»ƒè¿‡çš„æ‰¹æ¬¡ã€‚
    
    å·¥ä½œåŸç†:
    1. åŒ…è£…ä¸€ä¸ªåŸºç¡€é‡‡æ ·å™¨
    2. æŒ‰æ‰¹æ¬¡å¤§å°åˆ†ç»„
    3. è·³è¿‡å‰ skip_batches ä¸ªæ‰¹æ¬¡
    4. è¿”å›å‰©ä½™æ‰¹æ¬¡
    
    Attributes:
        sampler: åŸºç¡€é‡‡æ ·å™¨
        batch_size (int): æ‰¹æ¬¡å¤§å°
        skip_batches (int): è¦è·³è¿‡çš„æ‰¹æ¬¡æ•°
        
    Example:
        >>> sampler = DistributedSampler(dataset)
        >>> batch_sampler = SkipBatchSampler(sampler, batch_size=32, skip_batches=100)
        >>> loader = DataLoader(dataset, batch_sampler=batch_sampler)
        >>> # loader ä¼šè·³è¿‡å‰ 100 ä¸ªæ‰¹æ¬¡
    """
    
    def __init__(self, sampler, batch_size, skip_batches=0):
        """
        åˆå§‹åŒ–è·³è¿‡æ‰¹æ¬¡é‡‡æ ·å™¨
        
        Args:
            sampler: åŸºç¡€é‡‡æ ·å™¨ï¼ˆå¦‚ DistributedSamplerï¼‰
            batch_size (int): æ‰¹æ¬¡å¤§å°
            skip_batches (int): è¦è·³è¿‡çš„æ‰¹æ¬¡æ•°
        """
        self.sampler = sampler
        self.batch_size = batch_size
        self.skip_batches = skip_batches

    def __iter__(self):
        """
        è¿­ä»£æ‰¹æ¬¡
        
        Yields:
            list: æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        """
        batch = []
        skipped = 0
        
        for idx in self.sampler:
            batch.append(idx)
            
            if len(batch) == self.batch_size:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
                if skipped < self.skip_batches:
                    skipped += 1
                    batch = []
                    continue
                    
                yield batch
                batch = []
        
        # å¤„ç†æœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡
        if len(batch) > 0 and skipped >= self.skip_batches:
            yield batch

    def __len__(self):
        """
        è¿”å›æ‰¹æ¬¡æ€»æ•°ï¼ˆå‡å»è·³è¿‡çš„æ‰¹æ¬¡ï¼‰
        
        Returns:
            int: å®é™…è¿”å›çš„æ‰¹æ¬¡æ•°
        """
        total_batches = (len(self.sampler) + self.batch_size - 1) // self.batch_size
        return max(0, total_batches - self.skip_batches)
