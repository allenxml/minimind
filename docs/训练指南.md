# MiniMind 训练指南

本文档详细介绍 MiniMind 的完整训练流程。

---

## 目录

1. [训练流程概述](#训练流程概述)
2. [预训练](#预训练)
3. [监督微调](#监督微调)
4. [DPO 对齐](#dpo-对齐)
5. [推理蒸馏](#推理蒸馏)
6. [强化学习](#强化学习)
7. [知识蒸馏](#知识蒸馏)
8. [训练技巧](#训练技巧)

---

## 训练流程概述

MiniMind 采用多阶段训练流程：

```
预训练 → 监督微调(SFT) → DPO对齐 → 推理蒸馏/强化学习
   ↓           ↓            ↓              ↓
 语言能力    对话能力     人类偏好       推理能力
```

### 各阶段目标

| 阶段 | 目标 | 数据 | 学习率 |
|------|------|------|--------|
| 预训练 | 学习语言模型 | 纯文本 | 5e-4 |
| SFT | 学习对话能力 | 对话数据 | 5e-7 |
| DPO | 对齐人类偏好 | 偏好数据 | 4e-8 |
| 推理蒸馏 | 学习推理能力 | 推理数据 | 1e-6 |

### 推荐训练路径

**基础路径**（推荐新手）：
```
预训练 → SFT → 完成
```

**完整路径**（推荐进阶）：
```
预训练 → SFT → DPO → 推理蒸馏 → 完成
```

**强化学习路径**（实验性）：
```
预训练 → SFT → DPO → GRPO/SPO → 完成
```

---

## 预训练

### 目标

让模型学习语言的统计规律和基础知识。

### 数据格式

```json
{"text": "这是一段预训练文本..."}
{"text": "另一段预训练文本..."}
```

### 训练命令

```bash
# 基础预训练
python trainer/train_pretrain.py \
    --epochs 1 \
    --batch_size 32 \
    --learning_rate 5e-4 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --data_path ./dataset/pretrain_hq.jsonl
```

### 关键参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| epochs | 1-2 | 预训练通常 1-2 轮 |
| batch_size | 32-64 | 根据显存调整 |
| learning_rate | 5e-4 | 预训练可用较大学习率 |
| accumulation_steps | 8 | 梯度累积 |

### 训练时间估算

| 数据量 | GPU | 预计时间 |
|--------|-----|----------|
| 1M tokens | RTX 3090 | ~1 小时 |
| 10M tokens | RTX 3090 | ~10 小时 |
| 100M tokens | RTX 3090 | ~4 天 |

### 检查点

训练过程中会自动保存：
- `out/pretrain_512.pth` - 模型权重
- `checkpoints/pretrain_512_resume.pth` - 恢复点

---

## 监督微调

### 目标

让模型学会遵循指令和进行对话。

### 数据格式

```json
{
  "conversations": [
    {"role": "system", "content": "你是一个助手"},
    {"role": "user", "content": "你好"},
    {"role": "assistant", "content": "你好！有什么可以帮助你的？"}
  ]
}
```

### 训练命令

```bash
# 基于预训练权重进行 SFT
python trainer/train_full_sft.py \
    --epochs 2 \
    --batch_size 16 \
    --learning_rate 5e-7 \
    --from_weight pretrain \
    --data_path ./dataset/sft_mini_512.jsonl
```

### 关键参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| epochs | 1-3 | SFT 不宜过多轮 |
| learning_rate | 5e-7 | **比预训练小 100-1000 倍** |
| from_weight | pretrain | 基于预训练权重 |

### 损失计算

SFT 只计算 assistant 回复部分的损失：

```
用户: 你好          ← 不计算损失
助手: 你好！有什么... ← 计算损失
```

### 常见问题

**Q: 训练后模型变差了？**

A: 可能原因：
1. 学习率太大 → 降低到 1e-7
2. 训练轮数太多 → 减少到 1 轮
3. 数据质量差 → 检查数据格式

---

## DPO 对齐

### 目标

让模型的输出更符合人类偏好。

### 数据格式

```json
{
  "prompt": "如何学习编程？",
  "chosen": "学习编程可以从以下几个方面入手...",
  "rejected": "编程很难，你可能学不会..."
}
```

### 训练命令

```bash
python trainer/train_dpo.py \
    --epochs 1 \
    --batch_size 4 \
    --learning_rate 4e-8 \
    --beta 0.1 \
    --from_weight full_sft \
    --data_path ./dataset/dpo.jsonl
```

### 关键参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| learning_rate | 4e-8 | **极小学习率** |
| beta | 0.1 | 偏好强度，越小越激进 |
| batch_size | 4 | DPO 需要较小 batch |

### DPO 原理

DPO 通过对比学习优化模型：

```
损失 = -log(σ(β × (log π(chosen) - log π(rejected) - log π_ref(chosen) + log π_ref(rejected))))
```

### 注意事项

1. **学习率要极小**：避免模型遗忘
2. **需要参考模型**：训练时会加载两份模型
3. **显存需求翻倍**：因为需要参考模型

---

## 推理蒸馏

### 目标

让小模型学习大模型的推理过程。

### 数据格式

```json
{
  "conversations": [
    {"role": "user", "content": "1+1等于几？"},
    {"role": "assistant", "content": "<think>这是一个简单的加法...</think><answer>2</answer>"}
  ]
}
```

### 训练命令

```bash
python trainer/train_distill_reason.py \
    --epochs 1 \
    --batch_size 8 \
    --learning_rate 1e-6 \
    --from_weight dpo \
    --data_path ./dataset/r1_mix_1024.jsonl
```

### 特殊处理

推理蒸馏对特殊标签位置增加权重：

```python
# <think>, </think>, <answer>, </answer> 位置权重 ×10
loss_mask[special_token_positions] = 10
```

---

## 强化学习

### GRPO (Group Relative Policy Optimization)

```bash
python trainer/train_grpo.py \
    --epochs 1 \
    --batch_size 2 \
    --learning_rate 8e-8 \
    --num_generations 8 \
    --beta 0.02 \
    --reasoning 1 \
    --reward_model_path ./internlm2-1_8b-reward
```

### SPO (Self-Play Optimization)

```bash
python trainer/train_spo.py \
    --epochs 1 \
    --batch_size 2 \
    --learning_rate 1e-7 \
    --beta 0.02 \
    --reasoning 1 \
    --reward_model_path ./internlm2-1_8b-reward
```

### 奖励模型

需要下载奖励模型：
```bash
# 从 HuggingFace 下载
git lfs install
git clone https://huggingface.co/internlm/internlm2-1_8b-reward
```

---

## 知识蒸馏

### 目标

将大模型的知识迁移到小模型。

### 训练命令

```bash
python trainer/train_distillation.py \
    --epochs 6 \
    --batch_size 32 \
    --learning_rate 5e-6 \
    --student_hidden_size 512 \
    --student_num_layers 8 \
    --teacher_hidden_size 768 \
    --teacher_num_layers 16 \
    --alpha 0.5 \
    --temperature 1.5
```

### 关键参数

| 参数 | 说明 |
|------|------|
| alpha | CE 损失权重，蒸馏损失 = 1 - alpha |
| temperature | 蒸馏温度，越高标签越软 |

---

## 训练技巧

### 1. 显存优化

```bash
# 减小 batch_size
--batch_size 8

# 增加梯度累积
--accumulation_steps 4

# 使用 float16（节省显存但可能不稳定）
--dtype float16
```

### 2. 断点续训

```bash
# 自动检测并续训
python trainer/train_pretrain.py --from_resume 1
```

### 3. 多 GPU 训练

```bash
# 4 卡训练
torchrun --nproc_per_node=4 trainer/train_pretrain.py \
    --batch_size 8 \
    --accumulation_steps 2
```

### 4. 学习率调整

- 预训练：5e-4 ~ 1e-3
- SFT：5e-7 ~ 5e-6
- DPO：1e-8 ~ 5e-8
- LoRA：1e-4 ~ 5e-4

### 5. 监控训练

```bash
# 使用 wandb 记录
python trainer/train_pretrain.py --use_wandb --wandb_project MyProject
```

### 6. 数据质量

- 预训练：多样性 > 质量
- SFT：质量 > 数量
- DPO：对比要明显

---

## 常见问题

### Q: 训练 loss 不下降？

A: 检查：
1. 学习率是否合适
2. 数据格式是否正确
3. 模型是否正确加载

### Q: 训练后模型输出乱码？

A: 可能原因：
1. 学习率太大
2. 训练轮数太多
3. 数据有问题

### Q: 显存不足？

A: 解决方案：
1. 减小 batch_size
2. 增加 accumulation_steps
3. 使用更小的模型
4. 使用梯度检查点（需修改代码）

### Q: 如何评估模型？

A: 使用 eval_llm.py：
```bash
python scripts/eval_llm.py --weight full_sft