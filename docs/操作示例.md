# 操作示例大全

本文档提供 MiniMind 所有操作的完整示例，覆盖各种参数配置。

---

## 目录

1. [预训练示例](#1-预训练示例)
2. [监督微调示例](#2-监督微调示例)
3. [LoRA 微调示例](#3-lora-微调示例)
4. [DPO 对齐示例](#4-dpo-对齐示例)
5. [推理蒸馏示例](#5-推理蒸馏示例)
6. [GRPO 训练示例](#6-grpo-训练示例)
7. [SPO 训练示例](#7-spo-训练示例)
8. [知识蒸馏示例](#8-知识蒸馏示例)
9. [模型推理示例](#9-模型推理示例)
10. [分布式训练示例](#10-分布式训练示例)

---

## 1. 预训练示例

### 1.1 基础预训练

```bash
# 最简单的预训练命令
python trainer/train_pretrain.py
```

### 1.2 完整参数预训练

```bash
python trainer/train_pretrain.py \
    --save_dir ./out \
    --save_weight pretrain \
    --epochs 1 \
    --batch_size 32 \
    --learning_rate 5e-4 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 4 \
    --accumulation_steps 8 \
    --grad_clip 1.0 \
    --log_interval 100 \
    --save_interval 1000 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --max_seq_len 512 \
    --use_moe 0 \
    --data_path ./dataset/pretrain_hq.jsonl \
    --from_weight none \
    --from_resume 0
```

### 1.3 不同模型规格

```bash
# Small 模型 (26M)
python trainer/train_pretrain.py \
    --hidden_size 512 \
    --num_hidden_layers 8

# Base 模型 (104M)
python trainer/train_pretrain.py \
    --hidden_size 768 \
    --num_hidden_layers 16

# MoE 模型 (145M)
python trainer/train_pretrain.py \
    --hidden_size 640 \
    --num_hidden_layers 8 \
    --use_moe 1
```

### 1.4 断点续训

```bash
# 自动检测并续训
python trainer/train_pretrain.py \
    --from_resume 1 \
    --save_weight pretrain
```

### 1.5 使用 wandb 记录

```bash
python trainer/train_pretrain.py \
    --use_wandb \
    --wandb_project MiniMind-Pretrain
```

---

## 2. 监督微调示例

### 2.1 基础 SFT

```bash
python trainer/train_full_sft.py
```

### 2.2 完整参数 SFT

```bash
python trainer/train_full_sft.py \
    --save_dir ./out \
    --save_weight full_sft \
    --epochs 2 \
    --batch_size 16 \
    --learning_rate 5e-7 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 4 \
    --accumulation_steps 1 \
    --grad_clip 1.0 \
    --log_interval 100 \
    --save_interval 500 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --max_seq_len 512 \
    --use_moe 0 \
    --data_path ./dataset/sft_mini_512.jsonl \
    --from_weight pretrain \
    --from_resume 0
```

### 2.3 基于不同权重微调

```bash
# 基于预训练权重
python trainer/train_full_sft.py --from_weight pretrain

# 从头开始（不推荐）
python trainer/train_full_sft.py --from_weight none

# 基于已有 SFT 权重继续训练
python trainer/train_full_sft.py --from_weight full_sft --from_resume 1
```

### 2.4 不同数据集

```bash
# 使用 512 长度数据
python trainer/train_full_sft.py \
    --data_path ./dataset/sft_mini_512.jsonl \
    --max_seq_len 512

# 使用 1024 长度数据
python trainer/train_full_sft.py \
    --data_path ./dataset/sft_1024.jsonl \
    --max_seq_len 1024
```

---

## 3. LoRA 微调示例

### 3.1 基础 LoRA 训练

```bash
python trainer/train_lora.py
```

### 3.2 完整参数 LoRA

```bash
python trainer/train_lora.py \
    --save_dir ./out/lora \
    --lora_name lora_identity \
    --epochs 50 \
    --batch_size 32 \
    --learning_rate 1e-4 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 4 \
    --accumulation_steps 1 \
    --grad_clip 1.0 \
    --log_interval 10 \
    --save_interval 1 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --max_seq_len 512 \
    --use_moe 0 \
    --data_path ./dataset/lora_identity.jsonl \
    --from_weight full_sft \
    --from_resume 0
```

### 3.3 不同 LoRA 任务

```bash
# 身份认同 LoRA
python trainer/train_lora.py \
    --lora_name lora_identity \
    --data_path ./dataset/lora_identity.jsonl

# 医疗领域 LoRA
python trainer/train_lora.py \
    --lora_name lora_medical \
    --data_path ./dataset/lora_medical.jsonl

# 自定义 LoRA
python trainer/train_lora.py \
    --lora_name lora_custom \
    --data_path ./dataset/my_custom_data.jsonl
```

### 3.4 LoRA 超参数调整

```bash
# 更大的学习率（LoRA 可以用更大的学习率）
python trainer/train_lora.py \
    --learning_rate 5e-4 \
    --epochs 100

# 更多训练轮数
python trainer/train_lora.py \
    --epochs 100 \
    --save_interval 10
```

---

## 4. DPO 对齐示例

### 4.1 基础 DPO

```bash
python trainer/train_dpo.py
```

### 4.2 完整参数 DPO

```bash
python trainer/train_dpo.py \
    --save_dir ./out \
    --save_weight dpo \
    --epochs 1 \
    --batch_size 4 \
    --learning_rate 4e-8 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 4 \
    --accumulation_steps 1 \
    --grad_clip 1.0 \
    --log_interval 100 \
    --save_interval 500 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --max_seq_len 1024 \
    --use_moe 0 \
    --data_path ./dataset/dpo.jsonl \
    --from_weight full_sft \
    --from_resume 0 \
    --beta 0.1
```

### 4.3 DPO 超参数调整

```bash
# 更小的 beta（更激进的偏好学习）
python trainer/train_dpo.py --beta 0.05

# 更大的 beta（更保守的偏好学习）
python trainer/train_dpo.py --beta 0.2

# 极小学习率（避免遗忘）
python trainer/train_dpo.py --learning_rate 1e-8
```

---

## 5. 推理蒸馏示例

### 5.1 基础推理蒸馏

```bash
python trainer/train_distill_reason.py
```

### 5.2 完整参数推理蒸馏

```bash
python trainer/train_distill_reason.py \
    --save_dir ./out \
    --save_weight reason \
    --epochs 1 \
    --batch_size 8 \
    --learning_rate 1e-6 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 4 \
    --accumulation_steps 1 \
    --grad_clip 1.0 \
    --log_interval 100 \
    --save_interval 500 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --max_seq_len 1024 \
    --use_moe 0 \
    --data_path ./dataset/r1_mix_1024.jsonl \
    --from_weight dpo \
    --from_resume 0
```

### 5.3 不同基础权重

```bash
# 基于 DPO 权重（推荐）
python trainer/train_distill_reason.py --from_weight dpo

# 基于 SFT 权重
python trainer/train_distill_reason.py --from_weight full_sft
```

---

## 6. GRPO 训练示例

### 6.1 基础 GRPO

```bash
python trainer/train_grpo.py
```

### 6.2 完整参数 GRPO

```bash
python trainer/train_grpo.py \
    --save_dir ./out \
    --save_weight grpo \
    --epochs 1 \
    --batch_size 2 \
    --learning_rate 8e-8 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 1 \
    --accumulation_steps 1 \
    --grad_clip 1.0 \
    --log_interval 1 \
    --save_interval 10 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --max_seq_len 66 \
    --max_gen_len 1536 \
    --use_moe 0 \
    --data_path ./dataset/rlaif-mini.jsonl \
    --num_generations 8 \
    --beta 0.02 \
    --reasoning 1 \
    --reward_model_path ./internlm2-1_8b-reward \
    --from_resume 0
```

### 6.3 GRPO 关键参数

```bash
# 调整生成数量（每个 prompt 生成的样本数）
python trainer/train_grpo.py --num_generations 4

# 调整 KL 惩罚系数
python trainer/train_grpo.py --beta 0.01

# 普通模型（非推理模型）
python trainer/train_grpo.py --reasoning 0
```

---

## 7. SPO 训练示例

### 7.1 基础 SPO

```bash
python trainer/train_spo.py
```

### 7.2 完整参数 SPO

```bash
python trainer/train_spo.py \
    --save_dir ./out \
    --save_weight spo \
    --epochs 1 \
    --batch_size 2 \
    --learning_rate 1e-7 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 1 \
    --accumulation_steps 4 \
    --grad_clip 1.0 \
    --log_interval 1 \
    --save_interval 10 \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --max_seq_len 66 \
    --max_gen_len 1536 \
    --use_moe 0 \
    --data_path ./dataset/rlaif-mini.jsonl \
    --beta 0.02 \
    --reasoning 1 \
    --reward_model_path ./internlm2-1_8b-reward \
    --from_resume 0
```

---

## 8. 知识蒸馏示例

### 8.1 基础知识蒸馏

```bash
python trainer/train_distillation.py
```

### 8.2 完整参数知识蒸馏

```bash
python trainer/train_distillation.py \
    --save_dir ./out \
    --save_weight full_dist \
    --epochs 6 \
    --batch_size 32 \
    --learning_rate 5e-6 \
    --device cuda:0 \
    --dtype bfloat16 \
    --num_workers 4 \
    --accumulation_steps 1 \
    --grad_clip 1.0 \
    --log_interval 100 \
    --save_interval 500 \
    --max_seq_len 512 \
    --data_path ./dataset/sft_mini_512.jsonl \
    --student_hidden_size 512 \
    --student_num_layers 8 \
    --teacher_hidden_size 768 \
    --teacher_num_layers 16 \
    --use_moe 0 \
    --from_student_weight full_sft \
    --from_teacher_weight full_sft \
    --from_resume 0 \
    --alpha 0.5 \
    --temperature 1.5
```

### 8.3 蒸馏超参数调整

```bash
# 更高的蒸馏温度（更软的标签）
python trainer/train_distillation.py --temperature 2.0

# 更多 CE 损失权重
python trainer/train_distillation.py --alpha 0.7

# 更多蒸馏损失权重
python trainer/train_distillation.py --alpha 0.3
```

---

## 9. 模型推理示例

### 9.1 基础推理

```bash
python scripts/eval_llm.py
```

### 9.2 完整参数推理

```bash
python scripts/eval_llm.py \
    --load_from model \
    --save_dir out \
    --weight full_sft \
    --lora_weight None \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --use_moe 0 \
    --inference_rope_scaling \
    --max_new_tokens 8192 \
    --temperature 0.85 \
    --top_p 0.85 \
    --historys 4 \
    --device cuda
```

### 9.3 不同模型权重

```bash
# 预训练模型
python scripts/eval_llm.py --weight pretrain

# SFT 模型
python scripts/eval_llm.py --weight full_sft

# DPO 模型
python scripts/eval_llm.py --weight dpo

# 推理模型
python scripts/eval_llm.py --weight reason

# GRPO 模型
python scripts/eval_llm.py --weight grpo

# SPO 模型
python scripts/eval_llm.py --weight spo
```

### 9.4 使用 LoRA

```bash
# 身份认同 LoRA
python scripts/eval_llm.py --lora_weight lora_identity

# 医疗 LoRA
python scripts/eval_llm.py --lora_weight lora_medical
```

### 9.5 生成参数调整

```bash
# 更随机的生成
python scripts/eval_llm.py --temperature 1.0 --top_p 0.95

# 更确定的生成
python scripts/eval_llm.py --temperature 0.5 --top_p 0.7

# 更长的生成
python scripts/eval_llm.py --max_new_tokens 16384

# 多轮对话
python scripts/eval_llm.py --historys 6
```

### 9.6 长序列处理

```bash
# 启用 RoPE 外推（4倍长度）
python scripts/eval_llm.py --inference_rope_scaling
```

---

## 10. 分布式训练示例

### 10.1 单机多卡

```bash
# 4 卡训练
torchrun --nproc_per_node=4 trainer/train_pretrain.py \
    --batch_size 8 \
    --accumulation_steps 2

# 8 卡训练
torchrun --nproc_per_node=8 trainer/train_pretrain.py \
    --batch_size 4 \
    --accumulation_steps 4
```

### 10.2 指定 GPU

```bash
# 使用 GPU 0,1,2,3
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 trainer/train_pretrain.py

# 使用 GPU 4,5,6,7
CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nproc_per_node=4 trainer/train_pretrain.py
```

### 10.3 分布式 SFT

```bash
torchrun --nproc_per_node=4 trainer/train_full_sft.py \
    --batch_size 4 \
    --accumulation_steps 4 \
    --learning_rate 5e-7
```

### 10.4 分布式 DPO

```bash
torchrun --nproc_per_node=4 trainer/train_dpo.py \
    --batch_size 1 \
    --accumulation_steps 4 \
    --learning_rate 4e-8
```

---

## 附录：参数速查表

### 通用训练参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--save_dir` | str | `./out` | 模型保存目录 |
| `--save_weight` | str | 各脚本不同 | 权重名称前缀 |
| `--epochs` | int | 1-50 | 训练轮数 |
| `--batch_size` | int | 2-32 | 批次大小 |
| `--learning_rate` | float | 1e-8~5e-4 | 学习率 |
| `--device` | str | `cuda:0` | 训练设备 |
| `--dtype` | str | `bfloat16` | 混合精度类型 |
| `--num_workers` | int | 1-4 | 数据加载线程 |
| `--accumulation_steps` | int | 1-8 | 梯度累积步数 |
| `--grad_clip` | float | 1.0 | 梯度裁剪阈值 |
| `--log_interval` | int | 1-100 | 日志打印间隔 |
| `--save_interval` | int | 10-1000 | 保存间隔 |

### 模型架构参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--hidden_size` | int | 512 | 隐藏层维度 |
| `--num_hidden_layers` | int | 8 | 隐藏层数量 |
| `--max_seq_len` | int | 512 | 最大序列长度 |
| `--use_moe` | int | 0 | 是否使用 MoE |

### 推理参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--max_new_tokens` | int | 8192 | 最大生成长度 |
| `--temperature` | float | 0.85 | 生成温度 |
| `--top_p` | float | 0.85 | Top-p 采样 |
| `--historys` | int | 0 | 历史对话轮数 |
| `--inference_rope_scaling` | flag | False | RoPE 外推 |